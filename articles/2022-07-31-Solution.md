# 几则问题与排查思路

记录几个工作中遇到的问题。

### 自增主键溢出 & Log4j2 异步阻塞

凌晨，线上一个服务突然无法响应，所有接口 5xx，运维协助 dump JVM 快照后对服务进行了重启暂时恢复响应；

处理步骤：

- 查看工程日志，发现一组异步消费者抛出了主键溢出的异常，该组服务的作用是记录用户的行为记录，查明操作的数据表的主键类型为 int，所能容纳的数据量不大；

- 处理办法，在评估业务影响之后，决定新建一张临时表，为了能保存大量数据，顺便将主键类型改为 bigint，通过 rename 表来完成表的切换，然后再将旧表数据导回新表中，并重新消费 MQ，完成数据的补充解决了该问题；


在处理过程中也考虑了其他办法：

1. 使用 alter table table_name auto_increment = xxx 命令重新设置表的主键起始值，这个命令设置的新值必须比当前值大，所以无效；
2. 直接将 int 改为 bigint，根据以往的经验，对大表操作会导致锁表以及较大的主从延迟，于是不采用这个方法；

光是主键溢出不足以将服务变得无法响应，继续分析 JVM 栈信息，发现用于输出接口入参的 logger 被阻塞，所有 http 线程都阻塞在类似的位置；

再结合 JVM 堆信息，发现用于 log4j 使用的日志队列 Disruptor 满了，队列中存放信息都是数据表主键溢出的堆栈信息，原来是业务中为了监控异常，改造了一个 logger 用于将异常通过发邮件的方式告警，又由于每次发送邮件会有网络延迟，导致了 log 生产的速度远大于邮件发送速度，主键异常迅速将有界队列占满。

![key-overflow](https://github.com/notayessir/blog/blob/main/images/problem/key-overflow.png)

**当 disruptor 队列满时，log 生产端将会阻塞**，于是所有用到 logger 的接口都被阻塞。

后续使用 Prometheus 代替了这种告警方式，将业务与监控再进一步解耦。

### 重启导致的 Log4j2 日志丢失

业务中会将用户触发的某些事件打印到日志中，大数据组通过监听日志文件来收集该内容，某次反馈日志文件有丢失，于是开展了排查。

没有任何堆栈信息，因为之前异步日志出过问题，于是把排查思路往这个方向靠，结合大数据反馈的时间节点，发现该时间节点应用恰好重启过，再查看该类日志的 logger 定义，使用了异步输入，于是得出了大致的结论：该段时间进程内要处理的数据量较大，事件输入到 Disruptor 队列后即将刷盘到日志文件，此时服务被重启，日志没有刷盘成功，导致日志的丢失；

![error-sync](https://github.com/notayessir/blog/blob/main/images/problem/error-sync.png)

解决办法：将这类通过日志传递数据的 logger 改为同步输出，重新消费 MQ 完成该次数据补偿，因业务侧对数据进行了去重，重复消费不会造成影响； 

### 重启导致的 Redis 一组命令无法执行完整

业务中有一些服务有频率访问限制，例如规则为 1 分钟内最多只允许访问 5 次，某次接到客服反馈玩家访问达到上限并等待 1 分钟仍然不能访问，频率计数逻辑如下：

```java
// 对某个目标增加一次访问计数
public void incrCount(String target) {
    Long result = redisTemplate.opsForValue().increment(target, 1);
    if (result != null && result == 1) {
    		redisTemplate.expire(key, rateLimitRule.getTimeWindows(), TimeUnit.SECONDS);
    }
}
```

这段代码分两步完成对目标的计数：

1. 目标 +1；
2. 当是首次加 1 时，设置过期时间；

再结合客服反馈的问题现象，可以判断出现该问题的玩家只执行了步骤 1，步骤 2 却没执行；原因是当时修复了某个问题对服务进行了更新，导致部分玩家成功执行了步骤 1，想要执行步骤 2 时，服务重启，失去了设置超时的机会，目标后续无法再访问；

解决办法，使用 lua 脚本实现这个逻辑，确保原子性，Spring 内部的 RedisTemplate 也支持这类型的脚本拓展；



