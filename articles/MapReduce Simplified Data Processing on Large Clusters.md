原文：[MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf)

# MapReduce: Simplified Data Processing on Large Clusters

## 摘要

MapReduce 是一个编程模型，也是一种集处理和生成大型数据集的实现。MapReduce 模型中，用户定义预定义两个函数（方法）：

1. map 方法：接收一个入参键值对（状态 A），然后再借助键值对（状态 A）生成中间状态键值对（状态 B）；
2. reduce 方法：所有键值对中（状态 B），合并所有具有相同 key 的键值对（状态 B）；

像本论文展示的这样，现实中很多的任务系统都可以用这个模型去解释和实现。

在大规模的商用机器集群上执行用函数式编写的代码将使其自动地拥有并行能力。实现了这种算法程序的运行时系统有几个优势：

1. 精准的对输入数据进行分片；
2. 使用一组机器定时执行程序；
3. 能够处理机器宕机，管理集群内所需的通信功能；

这些特点能够让没有并行、分布式系统经验的编程人员容易的在大型分布式系统上使用资源。

我们在一个具有大规模、高拓展能力的商用集群上实现了 MapReduce：在数以千计的机器集群上使用 MapReduce 处理 TB 级别的数据。

编程人员会发现这系统很容易使用：每天跑在 Google 集群上的 MapReduce 任务有上千个（译者注：意味着可以很方便地对任务进行执行、更改）。

## 1 介绍

过去五年，作者和许多 Google 的同事已经实现了上百种不同计算类型、能够处理大量原始数据的功能系统，例如爬取海量的文档，海量的网络请求日志等等。为了计算各种派生类型的数据，例如翻转索引、多种以图为结构的网页文档、每个站点爬取的页面数量、获取某一天查询最频繁的数据集等等。其中大部分计算是要求直接得到结果的。但通常输入的数据很大，为了在合理的时间内得到结果，这些计算需要分布在上百甚至上千台机器上运行。这会产生一系列问题，如何并行计算，如何分散数据，如何处理失败。

为了解决这些复杂性，我们设计了一个新的抽象库，用来表达易于理解的计算，但隐藏了如并行化、容错、分布式数据以及负载均衡等混乱的细节。我们的抽象灵感来源于函数式编程语言如 Lisp 中的 map 的 reduce 函数。我们意识到大多数计算都会涉及这样一个处理流程：为了计算一组具有中间状态的键值对，将 map 函数应用到每一条逻辑记录上；然后为了恰当地合并派生数据，在所有具有相同键的值上应用 reduce 函数。这种用户能够自定义 map 和 reduce 的函数式模型，使我们能容易地进行并行计算，并且能以重新执行作为实现容错的重要机制。

完成这项任务最主要的贡献是一个简单又强大的接口，这个接口能够自动化并行和分布式计算，整合了这些功能的接口能够在大型分布式商用机器上取得高性能的表现。

剩余文章会分为以下几节来讲述，第二节讲述基本的编程模型并给出几个例子。第三节讲述基于集群环境的 MapReduce 实现。第四节讲述我们发现对编程模型有用的改进。第五节通过各样的任务来测试我们的实现。第六节会用我们的经验来探索 MapReduce 在生产索引系统的重写表现。第七节讨论相关内容以及未来的计划。

## 2 编程模型

计算过程接受一组键值对（A）作为输入，然后再产生一组键值对（B）。用户在使用 MapReduce 库时会使用两个函数，即 Map 和 Reduce。

Map 函数，由用户编写，接收一个键值对（A）并产生一组具有中间状态的键值对（B）。MapReduce 库会以具有中间状态的键值对的键 I 分组，然后传递给 Reduce 函数。

Reduce 函数，由用户编写，接收一个具有中间状态的键值对的键 I 以及键 I 对应的一组值。Reduce 函数会合并这些值并形成一组长度更小的值。调用 Reduce 函数之后通常会产生 0 或 1 个输出。用户的 Reduce 函数会遍历（具有中间状态的）键值对中的值，这允许我们在合适的内存中处理拥有海量值的列表。

### 2.1 举例

考虑在一组大型文件集合中对每个单词进行计数的问题。用户编写了如下伪代码：

```
/**
* @param key	文件名
* @param value  文件内容
*/
map(String key, String value):
	for each word w in value:
		EmitIntermediate(w, "1");

/**
* @param key    单词
* @param value  一组计数列表
*/
reduce(String key, Iterator values):
	int result = 0;
	for each v in values:
		result += ParseInt(v);
	Emit(AsString(result));
```

这段伪代码中，map 函数将每个单词计数为 1，并用中间状态类 EmitIntermediate 表示。reduce 函数针某一个单词的所有计数相加得到总数。

此外，用户编写代码将输入、输出文件以及可选的参数填充到 mapreduce 规范的对象中，然后传给 MapReduce 函数进行调用。用户所编写的代码与 MapReduce 库（用 C++ 实现）关联。附录 A 展示了这个例子的所有代码程序。

### 2.2 数据类型

前面的伪代码是以字符串类型来定义输入和输出，用户定义的 map 和 reduce 函数在概念上会对应如下类型：

| 函数名（参数）       | 返回类型    |
| -------------------- | ----------- |
| map (k1,v1)          | list(k2,v2) |
| reduce (k2,list(v2)) | list(v2)    |

输入的键值来自与输出键值不同的域名，进一步说，具备中间状态的键值对与输出的键值对来自同一个域名。（译者注：map 函数的输入和输出数据不一样，reduce 函数的输入和输出有关联性）。

C++ 的实现版本中，以字符串作为入参，输出的参数类型由用户定义，用户定义的函数可以将字符串转为合适的类型。

### 2.3 更多例子

下面列出几个有趣的例子来简单地说明 MapReduce 计算过程：

- 分布式字符串查询：（译者注：在文档中查询字符串）每当 map 函数匹配指定的模式时，都会得到一个匹配目标；reduce 作为恒等函数将 map 产生的具有中间状态的键值对作为结果直接输出。
- URL 访问频率计数：map 函数处理 web 页面的请求日志，输入中间状态键值对  \<url,1>；reduce 函数对具有同一个 url 的键值对进行合并计数，得到 \<url,count>。
- 反转页面-链接：（译者注：即一个链接出现在哪些页面上）map 函数处理每个页面包含的链接，输出 \<target,source>，其中 target 为链接，source 为页面地址；reduce 函数将具有相同 target 的值连接起来，得到 \<target, list(source)>。
- 每个域下的词向量：词向量总结了在一个页面或一组页面中，单词出现的频率，表示为 \<word, frequency>。map 函数处理输入的页面并产生 \<hostname, term vector>，(hostname 从页面中各个 URL 解析得出）；reduce 函数接收以页面为单位的词向量，将这些词向量合并，并抛弃出现频次少的词向量，最后得到 \<hostname, term vector>。
- 倒置索引：map 函数解析每个页面，生成一系列 \<word, document ID>；reduce 函数根据给出的单词，接收所有键值对，根据页面 id 进行分类，得到 \<word, list(document ID)>。输出结果形成一个简单的倒置索引，可以容易的更新计算过程并跟踪单词所在的位置。
- 分布式排序：map 函数从每条记录中提取键，输出 \<key,record>；reduce 函数不改变任何数据。这个计算过程取决于 4.1 节中描述的执行分区的设备性能以及 4.2 节中描述的属性排序。

## 3 实现

MapReduce 可能会有许多不同的实现，实现方式是否正确的取决于环境。比如，一种实现适合跑在小型基于内存共享的机器上，另一种实现则跑在拥有多处理器的大型 NUMA 上，还有其他实现跑在一组互联的机器中。

这节讲述一个在 Google 内广泛使用的计算环境实现：即基于以太网的大规模机器集群。环境如下：

1. 每台机器都运行 linux 系统，配置为 x86 架构，具有双核的处理器，2 至 4 GB 运行内存。
2. 网络带宽支持 100 mb/s 或 1gb/s，平均值少于总体带宽的一半。
3. 一个集群由上百或上千台机器组成，机器节点失败时很常见。
3. 存储由便宜的 IDE 硬盘组成，并直接挂载单个机器上。在内部开发了一个分布式文件系统用来管理储存在这些硬盘上的数据。这个文件系统使用复制的机制在不可靠的硬件上保证可用性和可靠性。
3. 用户提交任务到定时系统。每个任务由一组子任务组成，这些子任务由集群中的定时器映射到一组可用的机器上。

### 3.1 执行流程总览

Map 的调用是分布式的，输入的数据被自动地切分为 M 组，这些数据分片可以被不同的机器并行处理。Reduce 的调用也是分布式的，具有中间状态的键值对按照 key 被切分成 R 个分区（例如使用公式 hash(key) mod R 进行分区），分区数以及分区函数由用户自定义。

![Figure1](/Users/nuc/Desktop/Figure1.png)

图 1 展示了我们实现的 MapReduce 处理流程图。当用户调用 MapReduce 函数时，会出现下面一系列的处理流程（方便说明，下面序号对应图 1 的标记的序号）：

1. 用户代码里的 MapReduce 库将输入的数据文件切分大小为 16 MB - 64 MB 数量为 M 块的分片（也可由用户通过参数定义）。然后开始在集群的机器上启动多个用户副本程序。
2. 其中有一个副本是特殊的 master 副本，其余的为接受 master 分配任务的 woker 副本。整个任务中，有 M 个 map 任务与 R reduce 任务要分配。master 节点将一个 map 或 reduce 任务分配给一个集群中一个空闲的 worker 节点。
3. 一个被分配了 map 或 reduce 任务的 worker 节点会从对应的输入分片读取内容。该 worker 节点从输入分片中解析键值对然后传递键值对给用户定义的 Map 函数。Map 函数把具有中间状态的键值对生产到内存的缓存池中。
4. 缓存池的内容被分区函数分成 R 片，并被定期的写到本地磁盘上。缓存被写入本地磁盘后，对应存储的磁盘地址被回传给 master 节点，master 节点再将这些硬盘位置交给负责执行 reduce 的 worker 节点处理。
5. master 节点将这些硬盘位置分配给负责执行 reduce 任务的 worker 节点，reduce worker 使用远程过程调用（RPC）从之前执行 map 任务的 worker 节点的本地硬盘中读取数据（即被分片的键值对）。一个 reduce worker 节点读取对应的具有中间状态的键值对之后，会根据 key 进行排序，这样一来，具有相同 key 的键值对可以被分到对应的组中。一般来说，reduce 任务要处理含有多个不同 key 的数据，所有排序分组这个步骤是有必要的（译者注：应该是为了提高处理效率）。
6. 执行 reduce 的 worker 节点遍历被排过序的键值对，每遇到一个唯一的 key，就会将对应的这一组值传递给用户定义的 Reduce 函数处理。Reduce 函数处理后的数据会被添加到该分区的文件中。
7. 当所有的 map 和 reduce 任务完成后，master 节点唤醒用户调用，这时，MapReduce 调用交回到用户编写的代码。

整个大任务成功完成之后，mapreduce 会产生 R 份可用的输出文件（1 个 reduce 任务产生 1 份，文件名由用户定义）。一般来说，用户不需要去这 R 份输出文件整合汇成 1 份，而是被当作输入文件给另外的 MapReduce 任务调用，又或者给另外一个分布式应用使用，比如将这些输入文件又分区成多个文件。

### 3.2 Master 节点的数据结构

Master 节点维护几种不同的数据结构。为每个 map 和 reduce 任务保存执行状态（具有 idle，in-progress，completed 三种状态），以及每个 worker 节点的 id（非 idle 状态）。

master 节点是 map 任务与 reduce 任务之间传递键值对文件地址的管道。因此，master 节点会存储每个 map 任务完成之后产生的 R 个文件的地址以及文件大小，当 map 任务完成时，这些文件的地址和大小会被更新。master 节点会将这些信息逐步的推送到处于 in-progress 状态并执行 reduce 任务的机器上。

### 3.3 容错

从 MapReduce 被设计用来在上百上千台机器上处理大量数据时，这个库必须要能够优雅的应对机器宕机（失败）的情况。

#### Worker 节点失败

master 节点定期的维持到 worker 节点的心跳。如果在一定时间内没有收到来自 worker 节点的响应。master 节点将该 worker 节点标记为失败。任何在这台 worker 节点上完成的 map 任务都会被重置为 idle 状态，然后会定时安排运行在其他机器上。类似的，执行 reduce 任务失败的机器也是这样被处理。

之所以要重新执行在失败的 worker 节点上已完成的 map 任务，是因为这些相关的输出文件被保存在失败的 worker 节点上，这些 worker 节点此时是不可用的，无法获取已输出的文件。但在失败的 worker 上已完成的 reduce 任务却不一样，这些 reduce 任务无需重新执行，因为 reduce 的输入文件并不是存在本地磁盘上，而是存在一个全局的文件系统（译者注：reduce 的计算和存储是分开、独立的）。

当一个 map 任务首先由 worker A 执行后右被 worker B 执行（因为 A 节点失败了），所有正在执行 reduce 任务的 worker 都会收到 map 任务被重新执行的通知。任何还没有从 A 节点读取数据的 reduce 任务将会改为从 B 节点读取。

MapReduce 可以灵活的应对大规模 worker 节点失败。比如，在一个 MapReduce 执行过程中，因为网络维护，导致一个集群里的 80 台机器在一段时间内（几分钟）无法提供服务，master 节点重新执行这些无法提供服务的机器上已完成的任务，并且继续推进进度，最终完成整个 MapReduce 操作。

#### Master 节点失败

根据上面描述的 master 节点数据结构，master 节点能够容易的周期性写入检查点。如果 master 上的任务失败了，一个新的副本可以从最近的检查点恢复。考虑到只有一个 master 节点，它不太可能失败，如果 master 节点失败了，按照目前的实现，会中断该 MapReduce 计算过程。客户端可以检查这个情况并重试 MapReduce 操作。

#### 故障语义

当用户提供的 map 和 reduce 运算符是其输入值的确定性函数时，我们的分布式实现产生的输出与整个程序的无故障顺序执行产生的输出相同（译者注：确定性函数简单来说就是相同的数据多次输入到某个函数中，输出都是一样的）。

我们依靠 map 和 reduce 任务的原子提交来完成这个特性。每个处于 in-progress 的任务将它的输出写入到私有的暂存文件中。一个 reduce 任务产生一个文件，一个 map 任务产生 R 个文件。当一个 map 任务完成，该 worker 节点将携带 R 个文件的名称的信息发送给 master 节点，如果 master 重复收到对某个已经完成过的 map 任务的信息，master 会忽略这条重复的信息；如果是首次收到这个 map 任务的完成信息，matser 会将 R 份文件的名称记录在自身的数据结构中。

当一个 reduce 任务完成后，负责执行 reduce 的 worker 自动地将它的临时文件重命名为最终输出文件。如果相同的 reduce 任务在多台机器上被执行，意味着最终文件会被多次重命名，但最终文件不会有任何改变。我们依靠文件系统提供的原子性重命名来保证最终输出的文件只包含一次 reduce 任务得到的输出结果。

多数情况下，map 和 reduce 运算符是确定的，事实上我们的语义在这种情况下与顺序执行是相同的，这可以让编程人员容易地理解他们所编写程序的行为。当 map 或 reduce 运算符是非确定的，我们提供稍弱但仍具合理的语义。面对非确定性的操作符，特定的 reduce 任务 R1 所产生的输出与一个顺序执行的非确定性程序所产生的输出是相同的。不同的 reduce 任务 R2 所产生的输出与非确定性程序的不同顺序执行产生的R2的输出对应。

考虑这三个任务，map 任务 M、reduce 任务 R1、R2。让已提交的 Ri 的执行表示为 e(Ri)（正好有一个这样的执行），e(R1) 可能已经读取了 M 的一次执行产生的输出，而 e(R2) 可能已经阅读了 M 的另一次执行所产生的输出（译者注，不理解最后这两段的想要表达什么意思）。

#### 局部性

网络带宽在我们的计算环境中是比较稀缺的资源。我们借助将输入数据（被 GFS 管理）保存在集群中里本地硬盘的优势来节约网络带宽。GFS 将文件划分为 64 MB 大小的块，并在不同机器上保存一个块的多个副本（译者注：为了保证高可用）。MapReduce 中的 master 节点会考虑给一台机器分配与该机器存储的数据副本相关的 map 任务（译者注：优先就近原则，worker 节点被分配存储了哪些数据就执行对应的任务）。如果做不到这一点，它会尝试在这个副本的附近安排一个 map 任务（例如，被安排执行任务的机器与包含数据的机器位于同一网络交换机上）。当在集群中的大部分 worker 节点上运行大型 MapReduce 操作时，大多数输入数据都是本地读取的，不会消耗网络带宽。

#### 任务粒度

如上所述，我们将 map 细分为 M 块，将 reduce 分为 R 块。按照理想情况，M 和 R 在数量上应该远大于 worker 节点。让每个 worker 执行不同的任务能够提高动态负载均衡的能力，并且在遇到故障时，能加快任务恢复的速度；比如，一台已完成许多 map 任务的机器遇到故障时，集群能够将这些需要重新执行的任务传给其他 worker 节点。

如上所述，在我们的实现中，M 和 R 的大小有实际的边界，因为 master 需要做出 O(M+R) 个调度决策并在内存中存储 O(M\*R) 个状态（内存的因素占比较小，每个状态约为 1 Byte）。

进一步说，R 的大小受限于用户，因为每个 reduce 任务的输出结束于一个独立分开的输出文件。实际上，我们倾向于选择数量 M，这样每个任务的输入数据大约为 16 MB ~ 64 MB（对上节描述的局部性的优化也是最有效的），R 的大小是 worker 节点的 N 倍，N 不会很大。我们经常使用 2000 台 worker 节点执行 M = 200000 和 R = 5000 的 MapReduce 计算。

### 3.6 备份任务

延长 MapReduce 操作总时间的一个常见原因是“掉队”：整个 MapReduce 计算还差几个 map 或 reduce 任务就可以完成，但其中的某个任务在一台机器上花费了很长的时间。出现“掉队”的原因各种各样，比如，一台机器的硬盘时不时出现可纠正的故障，读的性能从 30 MB/s 降到 1 MB/s；集群调度系统可能将多个任务调度到一台机器上，导致它的 CPU、内存，硬盘、网络带宽资源出现竞争的情况，进一步使 MapReduce 执行得更慢。我们最近遇到的一个问题是，机器初始化代码时出现错误，导致处理器的缓存被禁用：受影响机器上的计算速度减慢了一百多倍。

我们有一个通用的机制来缓解“掉队”问题。当一个 MapReduce 操作接近尾声时，master 对剩下处于 in-progress 的任务进行备份执行，只要主执行或备份执行完成，该任务就会被标记为已完成。我们已经调整了这种机制，这个机制通常会增加计算资源的消耗，但不会很多，少于百分之几。

## 4 改进

虽然简单编写 Map 和 Reduce 函数提供的基本功能满足大多数需求，但我们发现一些扩展很有用，这一节让我们来描述这些内容。

### 4.1 Partitioning 函数

MapReduce 的用户可以指定他们想要的 reduce 输出文件数（R），数据通过这些任务被一个分区函数在具有中间状态的 key 上进行分割。MapReduce 默认提供哈希函数进行分区（例如 hash(key) mod R ）。这会倾向于得到相当均匀的分区。然而在一些情况下，对 key 使用其他分区函数进行数据分区也是有用的，例如输出的 key 是 URL，我们想把所有条目根据主机名输出到相同的文件中。为了支持这种情形，MapReduce 用户可以提供一个特殊的 Partitioning 函数，例如使用 hash(Hostname(urlkey)) mod R 作为分区函数，将来自同一主机名的所有 URL 输出到同一文件中。

### 4.2 顺序保证

我们保证对于一个给定的分区，具有中间状态的键值对会按照升序进行处理。这种排序保证使得生成每个分区的排序输出文件变得容易，也很有用，例如输出的文件格式需要支持根据 key 进行高效的随机查找，或者能让用户方便的对数据进行排序。

### 4.3 Combiner 函数

某些情况下，每个 map 任务产生的中间键存在大量重复，用户指定的 Reduce 函数是可交换的和关联的。一个例子是 2.1 节中的单词计数，单词频率倾向于遵循 Zipf 分布（译者注：齐夫定律），每个 map 任务会产生上百或上千形如 \<the, 1> 的键值对。所有的这些计数会被通过网络发送到一个 reduce 任务然后使用 reduce 函数汇总成一个数字。我们支持用户定义一个 Combiner 函数在数据被发送之前进行数据的合并操作。

Combiner 函数在执行 map 任务的机器上执行。一般来说，combiner 和 reduce 函数是由相同的代码实现，两者的区别是 MapReduce 如何处理函数的输出。reduce 函数将输出写到输出文件，combiner 函数则将输出写到中间文件发给 reduce 任务。

部分合并这个步骤显著地提高了某些类型进行 MapReduce 操作的速度。附录 A 展示了使用 combiner 函数的例子。

### 4.4 输入与输出类型

MapReduce 支持多种方式读取输入数据。例如文本格式的输入，每行当作一个键值对：key 为当前行所在文件的位置，value 为该行内容。另一种常见的格式是根据 key 排序的一系列键值对。每种输入模式的实现都知道该如何将内容分割成有意义的区间给 map 任务进行处理（例如文本模式的范围分割确保这种分割发生在以行为分界这个方向上）。用户可以通过实现 reader 接口来新增一种新的输入类型，也可以从 MapReduce 里预定义的输入类型中选择一种，大部分用户选择后者。

reader 不一定需要从文件里读取数据。比如，定义一个从数据库读取记录的 reader，又或者从内存中映射的数据结构中读取。

类似的，我们支持一组输出类型来生成不同格式的数据，用户也可以容易的添加新的输出类型。

### 4.5 副作用

一些情况下，MapReduce 用户发现可以方便的从 map 或 reduce 操作符产生辅助文件并当成额外的输出，我们依靠应用写入器来使这些副作用保持原子性和幂等性。一般来说，应用产生的数据会写入到一个临时文件，一旦文件完整的生成，会原子的对这个文件进行重命名。

我们没有提供对一个任务产生多个文件两阶段提交的支持，因此，产生具有跨文件一致性要求的多个输出文件的任务应该是确定性的，这种限制在实践中从来都不是问题。

### 4.6 跳过坏记录

有时候用户的代码里有 bug， 导致 Map 或 Reduce 函数在处理某些记录时崩溃。这样的 bug 阻碍 MapReduce 操作的完成，通常的做法是修复 bug，但有时候并不可行，也许 bug 出现在没有源码的第三方库里。有时候忽略一些记录也是可接受的，比如在大型的数据集上做统计分析。我们提供一个可选的执行模式，MapReduce 检测这些引起确定性崩溃的记录，为了推进执行进度跳过这些记录。

每个 worker 进程安装一个信号处理器用来捕获分段异常和总线错误。在调用用户的 Map 或 Reduce 操作之前，MapReduce 将连续的参数序列号保存在一个全局变量里，如果用户代码产生一个信号，信号处理器将包含连续序列号的“最后一口气”信息以 UDP 包发给 MapReduce 的 master 节点。当 master 节点看到一条记录上有超过一次故障的情况，它表明处理该条记录的 Map 或 Reduce 任务应该被跳过。

### 4.7 本地执行

在 Map 或 Reduce 函数中调试问题可能很棘手，因为当计算在分布式系统中开始时，通常是在具有上千台机器的环境里进行，任务分配到哪台机器由 master 动态的决策。为了方便的进行调试、概括、小规模测试，我们开发了一个可供替代的 MapReduce 库，能够在本地机器上连续执行 MapReduce 操作的所有工作。用户可以控制参与计算的 map 任务数。用户用特殊的标记调用他们的程序，然后可以容易地使用有用的调试或测试工具。

### 4.8 状态信息

master 节点运行一个内部 HTTP 服务器，并导出一组状态页面供用户使用。状态页面展示计算的进度，例如多少任务已完成，多少任务进行中，输入的字节大小，具有中间状态数据的大小，输出的字节大小，处理速率等等，页面上也包含每个任务产生异常和输出文件的跳转链接。用户可以使用这个数据去预估整个计算需要花费多少时间，是否需要投入更多的资源来参与计算。这些页面还可用于计算何时比预期慢得多。

另外，状态页面还展示了哪个 worker 节点失败了，map 或 reduce 任务失败时又被哪些节点处理了。这些信息在尝试在用户代码中诊断问题时很有用。

### 4.9 计数器

MapReduce 库提供一个计数器用来对出现的各种事件进行计数。例如，用户代码可能想要对已处理的单词进行计数，或者对德国文档索引进行计数等等。

为了使用这种功能，用户代码创建一个名为计数器的对象然后在 Map 或 Reduce 函数中适当的增加，比如下面这段代码：

```
Counter* uppercase;
uppercase = GetCounter("uppercase");	// 实例化计数器
map(String name, String contents):		// map 函数
	for each word w in contents:
		if (IsCapitalized(w)):
			uppercase->Increment();					// 计数 +1
		EmitIntermediate(w, "1");
```

每台 worker 节点定期地将计数器的值传给 master 节点（信息携带在 ping-pong 心跳检测中 ）。master 节点从成功执行的 map 和 reduce 任务中汇总计数器的值，然后在 MapReduce 操作完成时将这些值返回给用户。当前计数器的值也展示在 master 节点的状态页上，用户可以看到计算的进度。在汇总计数器的值时，master 节点消除了由同一个 map 或 reduce 任务重复执行得出的计数器值，从而避免重复计数（重复执行产生的原因来源于备份任务和失败时重试机制）。

MapReduce 库会自动维护一些计数器的值，例如处理过的键值对数量和产生的键值对数量。

用户发现计数器功能对检查 MapReduce 操作行为的合理性很有用。比如，在一些 MapReduce 操作中，用户的代码可能想确保产出键值对数量与处理过的输入键值对数量相等，又或者处理过的部分德国文件在所有文件总数中的某个可容忍部分内。

## 5 性能

这一节，我们通过两个跑在大规模集群上的程序来测量 MapReduce 的性能。一个程序是在大约 1TB 的数据中查找一个具体的模式，另外一个则是对大约 1TB 的数据排序。

这两个程序代表了 MapReduce 用户编写的真实程序的一大子集。一类程序将数据从一种表示形式转移到另一种表示形式，另一类则是从一个大数据集中提取感兴趣的一小部分数据。

### 5.1 集群配置

所有程序都在一个约由 1800 台机器组成的集群上执行。每台机器拥有双核主频为 2GHz Intel Xeon 支持多线程的处理器，4GB 运行内存，两个大小为 160 GB 的 IDE 硬盘，以及千兆以太网链路。这些机器被安排在一个两级树形交换网络中，在根节点具有大约 100-200 Gbps 的聚合带宽。所有机器处于同一个托管设施里，任意一对机器的往返时间都低于 1 毫秒。

除了 4 GB 的运行内存，集群中大约预留另外的 1 - 1.5 GB 内存供其他任务使用。这些程序在周末的下午被启动，此时的 CPU，硬盘，网络大多数处于闲置状态。

### 5.2 Grep

grep 程序扫描 10^10 条大小为 100 byte 的记录，查找相对稀少的包含 3 个字符的模式（这种模式出现在 92,337 条记录中）。输入会被分为大约 64 MB 的块（M = 15000）,整个输出被放到一个文件（R = 1）。

![Figure2](/Users/nuc/Desktop/Figure2.png)

图 2 展示了计算的进度。Y 轴表示扫描到的输入数据速率，速率根据越多的机器被分配用于参与 MapReduce 计算逐渐上升，当 1764 台 worker 被分配任务时，峰值超过 30 GB/s。当 map 任务结束，速度开始下降直到 0 约花了 80 秒。整个计算从开始到结束大约耗时 150 秒，这包括大约一分钟的启动开销。这个开销花费在将程序传播到所有 worker 机器上，从 GFS 中打开 1000 个文件的延迟，以及获取局部优化所需的信息。

### 5.3 Sort

sort 程序对 10^10 条大小为 100 byte 的记录排序（数据总大小约为 1 TB）。该程序以TeraSort基准为模型。该排序程序由不到 50 行代码组成。三行 map 函数从一行文本数据中提取 10 byte 大小的 key 然后生成以原始文本行作为值的具有中间状态的键值对。我们使用一个内置的 Identity 函数当作 Reduce 操作符，这个函数接收具有中间状态的键值对，不加更改地输出键值对。最终排序后的的输出被写到一组双向复制的 GFS 文件中（该程序输出的数据大小为 2 TB）。

在之前，输入的数据被分为 64 MB 大小的块（M = 15000），我们将排序后的输出分区为 4000 个文件（R = 4000），分区函数使用 key 的初始字节将其分隔为 R 个片段之一。

这个基准测试的分区函数内置了 key 分布的一些原理。在一般排序程序中，我们将添加一个预传递MapReduce 操作，该操作将收集 key 样本，并使用样本 key 的分布来计算最终排序传递的分割点。

![Figure3](/Users/nuc/Desktop/Figure3.png)

图 3(a) 展示了正常 sort 程序的执行进度。左上图显示了读取输入的速率。速率峰值达到 13 GB/s，然后在所有 map 任务结束之前的 200 秒相当快地消失。注意到输入速率相对 grep 来说较小，这是因为 sort 程序的 map 任务花费一半的时间和 I/O 带宽将具有中间状态的键值对写入到本地磁盘。grep 相应中间输出的大小可以忽略不计。

左中的图展示了数据从 map 任务通过网络发送到 reduce 任务的速率。当第一个 map 任务完成，洗牌立即开始。图表中的第一个驼峰代表了第一批约 1700 个 reduce 任务的速率（整个 MapReduce 计算约分配 1700 台机器，每台机器在同一时间最多执行一个 reduce 任务）。大约在计算开始 300 秒后，第一批 reduce 任务中的一些任务完成，我们开始为剩余的 reduce 工作洗牌数据，整个计算所需的洗牌时间约为 600 秒。

左下的图展示了 reduce 任务将排好序的数据写入到最终输出的文件速率。由于机器忙于对中间数据进行排序，所以在第一个洗牌周期结束和写入周期开始之间存在延迟。写入速率在一段时间内持续为 2 - 4 GB/s，所有的写入在计算中耗时约 850 秒，算上启动开销，整个计算耗时 891 秒。这与目前报告最好的 TeraSort 基准测试耗时的 1057 秒相似。

还有一些细节需要注意：由于本地优化，大部分数据从本地磁盘读取，并绕过我们相对带宽受限的网络，输入速率是高于洗牌速率和输出速率。洗牌速率高于输出速率因为输出阶段要写入两份已经排序的数据（写入两个副本是出于可靠性与可用性考虑）。写入两个副本是因为这是底层文件系统提供的可靠性和可用性，如果底层文件系统使用擦除编码而不是复制，则写入数据的网络带宽需求将减少。

### 5.4 后备任务的效应

在图 3(b) 中，展示了禁用后备任务功能的排序执行进度。执行流程类似于图 3(a)， 只是有一个很长的尾巴，几乎没有任何写活动发生。960 秒后，除了 5 个 reduce 任务外的所有任务都已完成，这些少数的拖延者直到 300 秒之后才完成，整个计算过程耗时 1283 秒，所用耗时增加了 44%。

### 5.5 机器宕机

在图 3(c) 中，展示了排序程序的执行，其中，我们故意在几分钟内从 1746 个 worker 进程中终止了 200 个进程。底层的集群调度马上在这些机器上重启了新的 worker 进程（只是进程被终止，机器正常运行）。

worker 的终结降低了输入速率，因为先前已完成的 map 工作消失了（对应的 map worker 被终止）并需要重新完成。这个 map 工作的重新执行相对较快，整个计算花费 933 秒，包括启动开销（相对正常的执行仅提高了 5% 的时间）。

## 6 经验

我们在 2003 年 2 月写了 MapReduce 库的第一版，然后在 2003 年 8 月进行了重要改进，包括 worker 机器之间执行任务时的本地优化，动态负载均衡等。从那时起，我们对 MapReduce 库能处理的问题的广泛适用性感到惊喜，它被用在 Google 的多个领域，包括：

- 大规模机器学习问题
- Google News 与 Froogle products 集群问题（例如 Google Zeitgeist）
- 提取用于生成流行查询报告的数据
- 新实验和产品网页属性的提取（从大型网页语料库中提取地理位置进行本地化搜索）
- 大规模图计算

![Figure4](/Users/nuc/Desktop/Figure4.png)

图 4 显示了随着时间的推移，检查到我们的主要源代码管理系统中的独立 MapReduce 程序数量的显著增长变化，2003 年初到 2004 年 9 月底，数量从 0 增长到近 900。MapReduce 之所以如此成功，是因为它可以在半小时内编写一个简单的程序并在一千台机器上高效运行，大大加快了开发和原型制作周期。更进一步来说，它能够让没有分布式或并行系统经验的编程人员容易的使用大量资源。

![Table1](/Users/nuc/Desktop/Table1.png)

 在每个 job 的结尾，MapReduce 库会打印每个 job 所使用的资源的统计。表 1 中，我们展示了 2004 年 8 月Google 运行的 MapReduce 作业子集的一些统计数据。

### 6.1 大规模索引

迄今为止，MapReduce 最重要的用途之一是完全重写了生产索引系统，该系统生成用于 Google web 搜索服务的数据结构。该索引系统将爬虫系统爬取到的大量文档作为输入，保存为一组 GFS 文件。这些文件的原生内容的数据量超过 20 TB。索引过程以五到十个 MapReduce 操作的顺序运行，使用 MapReduce （而不是以前版本的索引系统中的临时分布式传递）来完成这个操作有几个好处：

- 索引代码更简单、更小、更容易理解，因为 MapReduce 隐藏了容错、分布式、并行化的细节。例如当使用 MapReduce 时，某个步骤的计算实现从约 3800 行 C++ 代码减少到 700 行。
- MapReduce 库的性能足够好，我们可以将概念上无关的计算分开，而不是将它们混合在一起，以避免数据的额外传递。这样可以使它容易的修改索引流程，例如一个在旧索引系统中做一项修改需要耗时几个月，到了新系统中只需耗时几天。
- 索引过程变得更容易操作，因为大多数由机器故障、机器速度慢和网络故障引起的问题都由 MapReduce 库自动处理，无需操作员干预。更进一步，它可以容易的通过将新的机器添加到集群中来提高索引处理的性能。

## 7 相关工作

许多系统提供了受限编程模型，并使用这些限制自动并行计算。例如，使用并行前缀计算，可以在 N 个处理器上，在 logN 次的 N 元素数组的所有前缀上计算关联函数（译者注：这句待理解）。MapReduce 可以被认为是基于我们在大型现实世界计算中的经验，对其中一些模型的简化和提炼。更重要的是，我们提供了可扩展到数千个处理器的容错实现。相反，许多并行处理系统仅仅是小规模的实现，并把处理机器故障的细节留给编程者解决。

大多数同步编程和一些 MPI 原语提供了更高级的抽象，使程序员更容易编写并行程序。这些系统与 MapReduce 之间重要的区别在于 MapReduce 利用受限的编程模型来自动地并行化用户程序，还能提供透明的容错。

局部优化从活跃的磁盘之类的技术获得灵感，其中计算被推入接近本地磁盘的处理单元中，以减少通过 I/O 子系统或网络发送的数据量，我们在与少量磁盘直接连接的商用处理器上运行，而不是直接在磁盘控制器处理器上运行。

后备任务的机制类似于 Charlotte 系统中使用的急切调度机制。急切调度机制的一个缺点是如果给定的任务导致了重复失败，则整个计算无法完成。我们使用跳跃坏记录的机制来修复这个问题的一些实例。

MapReduce 的实现依赖于集群管理系统，集群管理系统负责在一组大型的共享机器中分发、运行用户的任务。尽管这不是这边论文的焦点，集群管理系统的设计思路上与其他系统例如 Condor 相似。

排序工具作为 MapReduce 库一部分在操作上类似于 NOW-Sort。源机器（map workers）对要排序的数据进行分区然后将它发送到 R 个执行 reduce 的 worker 节点上。每个 reduce worker 节点将它所需要处理的数据进行本地排序（如果可能的话，运行在内存中）。当然 NOW-Sort 不具备让用户自定义 Map 和 Reduce 函数的能力，基于这点，让我们的库被广泛应用。

River 提供了一种编程模型，进程通过分布式队列发送数据相互通信。与 MapReduce 一样，River 系统试图提供良好的平均性能，即使存在由异构硬件或系统扰动引入的非均匀性。River 通过仔细磁盘调度，以及网络传输来达成这个平衡完成时间这个目标。MapReduce 的实现有点不同，通过限制编程模型，MapReduce 框架可以将问题切分为很多细粒度的任务。这些任务被动态地调度在可用的 worker 机器节点上，一些运行得快的机器可以处理更多任务。受限的编程模型也允许我们在计算接近尾声时调度冗余的任务，这在存在非一致性的情况下大大缩短了完成时间（例如运行慢的 worker 或者 worker 因为某些原因被卡住）。

BAD-FS 与 MapReduce 的编程模型不同，不像 MapReduce 这样通过广域网执行任务。但仍有两个基础的相似点：

1. 两者都使用冗余的执行来恢复因为数据丢失引发的故障
2. 两者都使用位置感知调度来减少通过拥塞网络链路发送的数据量

TACC 是设计用来简化高可用网络服务的系统。如 MapReduce 这样，依赖重新执行这种机制来实现容错。

## 8 结论

MapReduce 编程模型已经成功的在 Google 内部被许多不同目的的应用使用。我们把这个成功归结成几个原因。第一，这个模型很容易使用，即使编程人员没有分布式、并发编程的经验，因为它隐藏了并行化、容错、局部优化、负载均衡这些细节；第二，许多问题可以很容易表示为 MapReduce 计算，例如，MapReduce 被用来给 Google 的产品网页搜索服务生成数据，还有排序、数据挖掘、机器学习和许多其他系统。第三，我们开发了一个 MapReduce 实现，可扩展到由数千台机器组成的大型机器集群。该实现有效利用了这些机器资源，因此适用于 Google 遇到的许多大型计算问题。

我们从这项工作中学到几个知识。第一，受限编程模型使其容易的进行并行化、分布式计算，以及容错。第二，网络带宽是稀缺的资源。一系列的优化就是为了减少通过网络发送的数据量：局部性优化让我们从本地磁盘读取数据，将具有中间状态的数据备份写到本地磁盘以节约网络带宽。第三，冗余执行可以减少运行较慢的机器产生的影响，以及能够处理机器故障和数据丢失问题。

## 致谢（省略）

## 引用（省略）

## A 词频例子（省略）