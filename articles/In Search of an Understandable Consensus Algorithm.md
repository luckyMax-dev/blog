原文：[In Search of an Understandable Consensus Algorithm](https://raft.github.io/raft.pdf)

# In Search of an Understandable Consensus Algorithm

## 摘要

Raft 是一个用来管理复制日志的共识算法。它与  (multi-)Paxos 产生的结果相同，并且比 Paxos 更有效率，但结构上会有所区别；这使 Raft 相对 Paxos 更易于理解，并为构建实际系统提供了更好的基础。为了加强易理解性，Raft 分离了共识的关键要素，例如领导选举，日志复制，安全性，它通过加强了一致性来减少必须考虑的一系列状态。用户研究的结果表明，Raft 比 Paxos 更容易让学生学习。Raft 还包括一种改变集群成员的新机制，它使用重叠多数来保证安全。

## 1 介绍

共识算法允许作为一个整体的一组机器在某些成员遇到失败时整个集群仍然能正常工作。基于此，它们在构建可信赖的大规模软件系统扮演重要的角色。过去十年，Paxos 在共识算法中占据了主要的讨论热度：大部分共识的实现都基于 Paxos 或者受其影响，Paxos 也成为向学生传授共识知识的主要工具。

在与 Paxos 斗争之后，我们开始寻找一个新的共识算法，它可以为系统地构建和教学提供更好的基础。我们的方法与众不同，因为我们的主要目标是可理解性：我们能否定义一个为实际的系统定义一个共识算法并将它简单地描述出来，相对于 Paxos 更易于学习？此外，我们希望算法有助于开发对系统构建者至关重要的直觉。重要的不仅仅是算法的工作，更重要的是它为什么工作。

这项工作的结果是一个被称为 Raft 的共识算法。在设计 Raft 时，我们应用了特定的技术来提高可理解性，包括分解（Raft 分为领导选举，日志复制与安全性）和状态空间的减小（与 Paxos 相比，Raft 减少了不确定性程度和服务器之间不一致的方式）。一项针对两所大学 43 名学生的用户研究表明，Raft 比 Paxos 更容易理解：学完两个算法之后，33 名学生能够回答 Raft 的问题比回答 Paxos 的问题更好。

Raft 在很多方面与现存的共识算法相似（最值得注意的是，Oki 和 Liskov 的 Viewstamped 复制），但它有几个新颖的特点：

- 强领导：Raft 使用比其他共识算法更强的领导形式。例如，日志数据只会从领导者流向其他服务器。这简化了对复制日志的管理并使 Raft 更容易理解。
- 领导选举：Raft 使用随机的定时器来选举领导。这仅为任何共识算法所需的心跳添加了少量机制，同时简单快速地解决冲突。
- 成员变更：Raft 用于改变集群中服务器集的机制使用了一种新的联合共识方法，其中两种不同配置的大多数在转换过程中重叠（译者注：这句话没理解）。者允许集群在配置改变期间能继续正常的运作。

我们相信 Raft 优于 Paxos 和其他共识算法，不管是出于教学目的还是基础实现。它相比其他算法更简单，更容易理解。它被描述得足够完整，以满足实际系统的需要；它有几个开源的实现，并且被一些共识所使用；它的安全性能已得到正式规定和证明；其效率与其他算法相当。

本文的其余部分介绍了复制状态机问题（第 2 节），讨论 Paxos 的优缺点（第 3 节），描述描理解性的一般方法（第 4 节），展示 Raft 共识算法（第 5 - 8 节），Raft 评估（第 9 节），讨论相关工作（第 10 节）。

## 2 复制状态机

共识算法通常出现在复制状态机的上下文中。在这种方法中，服务器集合上的状态机计算相同状态的相同副本，即使某些服务器关闭，也可以继续运行。复制状态机被用来解决在分布式系统中的各种容错问题。比如，拥有一个集群领导的大规模系统，如 GFS，HDFS，RAMCloud，通常使用单独的复制状态机来管理领导者选举，并存储必须在领导者崩溃后仍然存在的配置信息。复制状态机的例子包括 Chubby 和 ZooKeeper。

![Figure1](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure1.png)

*图 1 注：复制状态机架构。共识算法管理一个包含来自客户端的状态机命令。状态机处理来自日志里相同序列的命令，所以它们产生同样的输出。*

复制状态机一般使用复制日志实现，如图 1 展示。每台服务器存储一个包含一系列按序执行命令的日志。每个日志包含相同顺序的相同的命令，所以每台状态机处理相同序列的命令。由于状态机是确定性的，每个状态机计算相同的状态和相同的输出序列。

保持复制日志的持久化是该共识算法的工作。服务器上的共识模块接收来自客户端的命令并将命令添加到它的日志里。它使用共识模块与其他服务器交互，来确保每条日志包含相同的按照顺序排列的请求，即使有一些服务器遇到故障。一旦命令被正确地复制，每台服务器的状态机按照日志顺序处理它们，并将输出返回给客户端。因此，服务器似乎形成了一个单一的、高度可靠的状态机。

共识算法在实际系统中一般有如下几个特点：

- 它们在非拜占庭条件下保证安全性（不会返回错误的结果），包括网络延迟、分区、数据包丢失、复制和重新排序。
- 只要大多数服务器都可以运行，并且可以相互通信和与客户机通信，它们就可以完全发挥功能（可用）。因此，由五台服务器组成的典型集群可以容忍任何两台服务器的故障。假设服务器因为停止而失败，它们稍后会通过存储的状态恢复并重新加入集群。
- 它们不依赖于时间来确保日志的一致性：错误的时钟和极端的消息延迟在最坏的情况下会导致可用性问题。
- 在一般情况下，只要集群中的大多数响应了单轮远程过程调用，命令就可以完成；少数较慢的服务器不会对整体系统的性能造成影响。

## 3 Paxos 的问题

过去十年，莱斯利·兰伯特的 Paxos 协议几乎成为共识的代名词：课程里被教得最多的协议，许多共识的实现都以它为起点。Paxos 首先定义了一个对一个单一决策达成一致的协议，例如一个单一复制日志条目。我们将该子集称为单一法令 Paxos。Paxos 组合该协议的多个实例来促进一系列的决策例如一条日志（multi-Paxos）。Paxos 确保安全性和活力，它支持集群内的成员变化。它的正确性已经被验证并且在常见的案例中也很有效率。

不幸的是，Paxos 有两个显著的缺点。第一个缺点，Paxos 是出了名地难以理解。完整的说明是出了名的不透明；很少有人能成功地理解它，只有付出巨大的努力。结果是以及有几次尝试用简单的术语去解释 Paxos。这些解释说明聚焦在单一法令子集，但仍然具有挑战性。在 2012 年 NSDI 的一项非正式调查中，我们发现很少有人对 Paxos 感到满意，即使是经验丰富的研究人员。我们同样也和 Paxos 做斗争，直到我们阅读了一些简化的解释并设计了我们自己的替代方案后，我们才能够理解完整的方案，这个过程花了将近一年的时间。

我们假设 Paxos 的不透明性来源于它选择单一法令子集作为其基础。单一法令 Paxos 浓密而微妙：它被分为两个阶段，没有简单直观的解释，无法独立理解。因此，很难对单一法令协议的工作原理产生直觉。multiPaxos 的组合规则增加了显著的额外复杂性和微妙性。我们认为，就多个决策达成共识的总体问题（即，一个日志而不是一个条目）可以通过其他更直接和更明显的方式进行分解。

第二个缺点，Paxos 没有提供一个好的基础来构建实际实现。一个原因是，对于 multi-Paxos，没有广泛一致的算法。兰伯特的表述更多是关于 single-Paxos；他概述了接近 multi-Paxos 的可能性，但许多细节是缺失的。曾经有几次尝试去详细解释和分析 Paxos 算法，例如 [26]，[39]，[13]，但这些有区别于兰伯特的表述。诸如 Chubby 系统已经实现了类似 Paxos 的算法，但更多的细节没有公布出来。

进一步，Paxos 的架构是简陋的，构建现实系统很乏力；这是单一法令分解的另一个结果。例如，独立选择一组日志条目，然后将它们合并成一个顺序日志，没有什么好处；这徒增复杂度。围绕日志设计一个系统更简单和有效，新条目按照受限的顺序连续地添加。另一个问题是 Paxos 在其核心使用了对称点对点方法（尽管它最终建议使用一种弱的领导形式作为性能优化）。这在一个简化的世界中是有意义的，在这个世界中，只需要做出一个决策，但很少有实际系统使用这种方法。如果需要做出一系列的决策，首先选举领导人，然后让领导人协调决策，这样更简单、更快速。

因此，实际系统与 Paxos 几乎没有相似之处。每个实现始于 Paxos，在实现它的过程中发现问题，然后开发了完全不同的架构。这既耗时又容易出错，理解 Paxos 的困难加剧了这个问题。Paxos 公式可能是证明其正确性定理的好方法，但实际的实现与 Paxos 有很大的不同，因此证明没有什么价值。下面来自 Chubby 的实现者的评论很典型：

> Paxos 算法和现实系统的描述有很大的跨度......最终的系统会基于一个没有被证明过的协议。

因为这些问题，我们做出结论，Paxos 没有给构建系统或教学提供一个好的基础。考虑到共识在大型软件系统中的重要性，我们决定看看我们能否设计一种比 Paxos 性能更好的可替代的共识算法。Raft 就是该实验的结果。

## 4 可理解的设计

我们在设计 Raft 算法时有几个目标：它必须提供一个完整的和实用的基础来构建系统，目的是大大减少开发者的设计工作；它在所有情况下必须是安全的，在各种操作条件下是可用的，对于常见操作它还必须是有效率的。但我们最重要的目标也是最难的挑战是，可理解性。大量观众必须能够轻松地理解算法。此外，它还必须有可能培养对算法的直觉，目的是系统建设者可以进行在实际实现中不可避免的扩展。

在 Raft 算法中有许多设计的点，我们必须在备选方案中进行选择。在这些情况下，我们基于可理解性进行评估选择：解释每一种选择有多难（比如状态空间是否复杂，它是否有微妙的含义？），读者能够容易和完整的理解这些方法以及它的含义？

我们认识到，这种分析具有高度的主观性；尽管如此，我们使用了两种普遍适用的技术。第一种技术是广为人知的问题分解方法：尽可能将问题分解成子问题，这些子问题可解释，可解决，以及相对独立地理解。比如，在 Raft 中我们分成了领导选举，日志复制，安全性以及成员变化的字问题。

第二个方法是通过减少一定数量要考虑的状态简化状态空间，尽可能使系统更连贯并消除不确定性。特别地，日志不允许存在空洞，Raft 限制了日志出现彼此不一致的情况。尽管在多数情况下我们尝试去消除不确定性，但在某些情况下，不确定性实际上提高了可理解性。特别是随机方法引入了不确定性，但它们倾向于通过以类似的方式处理所有可能的选择来减少状态空间（“随便选一个，没关系”），我们使用随机化来简化 Raft 的领导选举算法。

## 5 Raft 共识算法

Raft 是一个如第 2 节描述的那样用来管理复制日志的算法。图 2 以简明形式总结了算法，以供参考；图 3 列举了算法的重要特性；这些图的元素将在本节的其余部分逐段讨论。

![Figure2](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure2.png)

*图 2 注：Raft 共识算法的简要概述（排除成员变化和日志压缩）。左上框中的服务器行为被描述为一组独立且重复触发的规则，章节序号如 5.2 表示特定的特点在哪里被讨论。一个正式的说明更精确地描述了算法。（译者注：图片内容暂不翻译）*

![Figure3](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure3.png)

*图 3 注：Raft 保证这些特性全程生效。章节序号表示这些属性分别在哪里被展开讨论。（译者注：图片内容暂不翻译）*

Raft 通过首先选择一位杰出的领导者，然后让领导者全权负责管理复制日志来实现共识。领导者接受来自客户端的日志条目，将它们复制到其他服务器上，然后告诉这些服务器合适安全的将日志条目应用到它们的状态机。引入一个领导者简化了复制日志的管理。比如，领导者可以不用与其他服务器沟通就可以决定将新的日志条目放在哪里。领导者可能会出现故障或与其他服务器断开连接，在这种情况下，会选择新的领导者。

给定领导者方法，Raft 将共识问题分解为三个相对独立的子问题，在以下小节中讨论：

- 领导选举：一个新的领导必须是在一个已存在的领导遇到失败时才能被选举（5.2 小节）。
- 日志复制：领导必须接受来自客户端的日志条目并且通过集群复制它们，强制其他日志与自己的日志一致（5.3 小节）。
- 安全性：Raft 关键安全特性是图 3 中的状态机安全特性：如果任意服务器将某条日志条目应用到它的状态机，其他服务器可能将一个不同的命令应用到相同的日志索引上。5.4 小节，描述了 Raft 如何确保这个属性；解决方案涉及一个额外的在 5.2 节描述的选举机制限制。

在展示共识算法之后，本节讨论系统中的可用性问题和计时的作用。

### 5.1 Raft 基础

一个 Raft 集群包含几个服务器；典型值为 5，它允许系统中两个节点失败。在任意一个时间，每个服务器处于三个状态中的一个：领导者，跟随者，候选者。正常运行的情况下，只有一个领导者，其余均为跟随者。跟随者是被动的：它们不会由自己产生请求但会简单地响应来自领导者和候选者的请求。领导者处理所有客户端的请求（如果客户端与一个跟随者联系，该跟随者会将其重定向到领导者节点上）。第三个状态是候选者，如 5.2 节描述那样，用来选举新的领导者。图 4 展示了它们状态之间的转变；该转变如下讨论。

![Figure4](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure4.png)

*图 4 注：服务器状态。跟随者只响应来自其他服务器的请求。如果一个跟随者没有收到任何交流信息，它会成为一个候选者然后初始化选举。一个候选者接收来自集群中大多数节点的投票并成为新的领导者。领导者通常运行至它们遇到故障失败。*

Raft 将时间划分为任意长度的任期，如图 5。任期由数字表示，并且数字是连续的整型。每个任期起始于一次选举，一个或多个候选者尝试成为领导者，如 5.2 节所述。如果一个候选者赢得了选举，它就如领导者一样在剩余任期时间内提供服务。在一些情况下一个选举会导致分裂的投票。这些情况下会导致没有领导者；一个新的任期（伴随新的选举）将快速地开始。Raft 确保在任意一个任期内只有最多一个领导者。

![Figure5](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure5.png)

*图 5 注：时间被分开为任期，每个任期起始于一个选举。在一个成功的选举之后，一个单独的领导者管理整个集群直到任期结束。一些选举可能会失败，这种情况下不会选出领导者。这些任期间的转换可能在不同时期的不同服务器上被观察到。*

不同的服务器可以在不同的时间观察任期之间的转换，某些情况下，一台服务器可能察觉不到一次选举甚至整个任期。在 Raft 算法中，任期表现得像一个逻辑时钟，它们允许服务器检测如陈旧的领导者这类过期的信息。每台服务器保存一个当前的任期序号，任期序号随着时间单调递增。每当服务器通信时，就会交换当前任期；如果一个服务器的当前任期小于其他服务器，那它会将自己当前任期更新到更大的那个值。如果一个候选人或者领导者发现它的任期过期了，它会马上恢复为跟随者状态。如果一个服务器接受到一个带着陈旧任期的请求，它会拒绝请求。

Raft 服务器使用远程过程调用（RPC）进行通信，基础的共识算法只需要 2 种类型的 RPC。请求投票 RPC 在选举期间被候选者初始化（5.2 小节），AppendEntries RPC 由领导者初始化用来复制日志条目并提供一种形式的心跳（5.3 小节）。章节 7 增加了一个在服务器之间传输快照的第三个 RPC。如果服务器没有及时收到响应，则会重试 RPC，并行发出 RPC 以获得最佳性能。

### 5.2 领导选举

Raft 使用一个心跳机制来触发领导选举。当服务器启动时，它们以跟随者作为起始点。只要服务器从领导者或候选者处接收到有效的 RPC，服务器就会保持跟随者状态。领导者发生周期的心跳（AppendEntries RPC 可以不携带具体条目，视为心跳）给所有跟随者，目的是维持它们的权威。如果一个跟随者没有收到周期的通信，这种情况称为选举超时，它会假定没有可行的领导人，开始选举以选出新的领导人。

为了开始一次选举，跟随者增加它当前的任期并转变为候选者状态。然后它为自己投上一票并且并行地给集群中的其他服务器发出请求投票 RPC。候选者会一直保持它的状态直到下面三件事发生：(a)赢得选举；(b)另外一个服务器把自己当选为领导者；(c)在一段时间内没有胜出者。这些结论会在下面的片段分开讨论。

一个候选者如果收到整个集群在同一任期的多数投票，那么它就算赢得该次选举。每台服务器在给定的任期内，最多给一个候选者投票，基于先到先得的原则（注释：5.4 小节增加一个额外的投票限制）。多数原则确保最多一名候选人能在特定任期内赢得选举（图 3 中的选举安全特性）。一旦一个候选者赢得一次选举，它就成为领导者。它然后开始发送心跳信息给其他所有服务器，用来建立自己的权威并防止产生新的选举。

在等待投票期间，候选者可能收到来自其他服务器表明自己想要成为领导者的 AppendEntries RPC。如果领导者的任期（包括在其 RPC 中）至少与候选人的当前任期一样大，则候选人承认领导者是合法的，并返回到跟随者状态。如果 RPC 里的携带的任期小于候选者的当前任期，候选者拒绝该 RPC 并保持候选者状态。

第三个可能的结果是候选者即没有赢得选举也没有失去选举：如果许多跟随者在同一时间成为候选者，投票可能分裂，导致没有候选者赢得大部分选票。当出现这种情况，每个候选者会超时并且通过增加它们的任期和初始化另外一轮的请求投票 RPC 来开始新的一轮选举。然而，如果没有额外的措施，选票分裂可能会无限期重复。

Raft 使用随机化的选举超时来确保分裂投票很少出现并且及时出现也能快速解决。首先为了防止选票分裂，选举超时的时间从一段固定的区间随机选择（例如从 150 - 300 毫秒这个区间）。这扩展了服务器，因此在大多数情况下，只有一台服务器超时；它赢得选举并且在其他服务器超时之前发送心跳。相同的机制被用来处理选票分裂。每个候选人在选举开始时重新启动其随机选举超时，并等待超时时间过去后再开始下一次选举；这减少了在新一轮选举出现选票分裂的可能性。9.3 小节展示了这种方法可以快速选举一个领导者。

选举是一个例子，说明了可理解性如何指导我们在设计方案之间的选择。最初，我们打算使用排名系统：每个候选者被分配一个唯一的排名，该排名被用来从竞争中的候选者中选择。如果一个候选者发现另外一个有更高排名的候选者，它将返回到跟随者状态，这样有更高排名的候选人可以更容易的赢得下次选举。我们发现这种方法在可用性方面产生了一些微妙的问题（一个有低排名的服务器在遇到一个具有高排名的服务器失败时，可能需要超时并再次成为一个候选者，但如果这种情况发生得很快，它可能会重新设定选举领导人的进程）。我们几次对算法做出了调整，但每次调整之后都会遇到新的情况。最终我们得出了这个随机重试的方法，该方法更明显，更容易理解。

### 5.3 日志复制

一旦一个领导被选举出来，它开始为客户端的请求提供服务。每个客户端的请求包含一条即将被复制状态机执行的命令。领导者将命令当作新的条目附加到它的日志里，然后并行地发出 AppendEntries RPC 给其他服务器，其他服务器复制该附加日志。当条目被安全地复制（如下描述），领导者将条目应用到它的复制状态机并且返回执行的结果给客户端。如果跟随者崩溃或者运行得很慢，或者网络包丢失，领导者无限重试 AppendEntries RPC（即使在它响应客户之后）直到所有跟随者最终存储了所有的日志条目。

![Figure6](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure6.png)

*图 6 注：日志由按顺序编号的条目组成。每个条目包含被创建的任期编号以及一条给状态机的命令。一个条目被认为是已提交的，如果它被状态机安全地应用。*

日志的组织方式如图 6 所示。领导者接收的日志条目，每个日志条目保存的状态机命令都会携带任期编号。日志条目里的任期编号被用来在日志之间检测不一致性并确保图 3 中的一些特性。每个日志条目还具有一个整数索引，用于标识其在日志中的位置。

领导者决定何时将日志条目应用于状态机是安全的；这种条目被称为已提交。Raft 算法保证所有已提交的条目都是持久化的并且最终会被所有可用的机器执行。一旦领导者创建了条目并将其复制到集群中的多数节点（例如条图 6 中的条目 7 ），那么说明该条日志条目是已提交的。这也会提交领导者日志中的所有之前的条目，包括由前领导者创建的条目。5.4 小节讨论了领导者变更后应用该规则时的一些微妙之处，这也表明，先前承诺的定义是安全的。领导者跟踪其知道要提交的最高索引，并将该索引包含在未来的 AppendEntries RPC 中（包括心跳），以便其他服务器最终发现。一旦一个跟随者知道了一条日志条目已经提交，它就会将该条目应用到自己本地的状态机上（按照日志的顺序）。

我们设计了 Raft 日志机制，以保持不同服务器上的日志之间的高度一致性。不仅仅是简化系统行为和使其变得可预测，更重要的是它是确保安全性的重要组件。Raft 维持了以下特性，这些特性一起组成了如图 3 中的日志匹配特性：

- 如果两个条目在不同日志中有相同的索引和任期，则它们存储的是相同的命令。
- 如果两个条目在不同日志中有相同的索引和任期，则在该条目之前的所有条目都是一致的。

第一个特性遵循了一个事实，事实是一个领导者在给定的任期给定的日志索引最多创建一个条目并且日志条目从不改变它们在日志里的位置。第二个特性是由 AppendEntries 支持的简单一致性检查来保证。当发送AppendEntries RPC 时，领导者在其日志中包含新条目之前的条目的索引和任期。如果跟随者在它的日志里没有发现具有相同索引和任期的条目，那么它会拒绝该新条目。一致性检查作为一个导入步骤：日志的初始空白状态满足日志匹配特性，每当扩展日志时，一致性检查都会保留日志匹配属性。最后结果是，无论何时 AppendEntries 成功地返回，领导者都知道跟随者的日志与新条目中的自己的日志相同。

在正常操作期间，领导者的日志和跟随者保持一致，所以 AppendEntries 的一致性检查从不会失败。然而，领导者崩溃会引发日志非一致性问题（旧的领导者可能没有完整地复制它日志里的所有条目）。这些非一致性会随着一系列领导者和跟随者崩溃而加剧。图 7 阐述了几种跟随者的日志可能不同于新领导者的方式。一个跟随者可能缺少在领导者上出现的条目，也可能拥有领导者上没有出现的条目，或者两种情况都出现。在日志里缺少条目或者出现无关条目的情况可能会跨越几个任期。

![Figure7](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure7.png)

*图 7 注：当最高层的领导者掌权时，有可能在追随者日志中出现任何场景（a–f）。每个方块代表一个日志条目；在方块里的数字代表任期。跟随者可能缺失条目（a-b），可能拥有额外未提交的条目（c-d）或（e-f）。比如，场景（f）可能出现是因为该服务器在任期 2 是领导者，添加几个条目到它的日志里然后在提交它们之前崩溃了；之后迅速重启，成为任期 3 的领导，又新增了几个条目到它的日志里；在任何属于任期 2 或任期 3 的条目被提交之前，该服务器再次崩溃并在几个任期内保持宕机的状态，这就是场景（f）可能出现的原因。*

在 Raft 中，领导者使用强制跟随者复制自己的日志来处理非一致性。这意味着跟随者日志里冲突的条目会被领导者的日志覆盖写入。5.4 小节将表明，当再加一个限制时，这是安全的。

为了使跟随者的日志与自己的日志保持一致，领导者必须找到两个日志一致的最新日志条目，删除跟随者该条目所在位置之后的所有条目，然后发送领导者节点上该条目之后的所有条目给跟随者。所有这些操作都是为了响应 AppendEntries RPC 执行的一致性检查而执行的。领导者为每个跟随者维护一个 nextIndex 变量，代表着领导者发送给跟随者的下一个日志条目索引。当一个领导者第一次掌权时，它初始化所有 nextIndex 变量为它当前日志所在的索引值（图 7 中，索引值为 11）。如果一个跟随者的日志与领导者的不一致，AppendEntries 一致性检查会在下一个 AppendEntries RPC 失败。在一次拒绝之后，领导者减少 nextIndex 并重试 AppendEntries RPC。最终 nextIndex 将到达领导者与跟随者日志匹配的位置。当遇到这个情况，AppendEntries 将会成功，并移除跟随者日志中冲突的条目，然后从领导者的日志中添加条目。一旦 AppendEntries 成功，跟随者的日志与领导者的一致，之后也会一直保持这种方式同步剩余的任期。

如果需要，可以优化协议以减少被拒绝的 AppendEntries RPC 的数量。比如，当遇到拒绝 AppendEntries 的请求，跟随者可以包括冲突条目的任期和它为该任期存储的第一个索引。有了这个信息，领导者可以减少 nextIndex 变量来绕过该任期内所有冲突的条目；相较于一个 RPC 对应一个条目，一个任期内冲突的条目仅需要一个 AppendEntries RPC。实际上，我们考虑这个优化是否有必要，因为失败的情况发生得不频繁并且也不太可能存在很多不一致的条目。

有了这个机制，当一个领导者掌权时，不需要采取额外的动作来恢复日志一致性。它只需要开始正常的操作，日志以响应 AppendEntries 的一致性检查失败而自动聚集。一个领导者从来不会重写它的日志或者从中删除条目（图 3 中描述的领导者仅附加属性）。

这个日志的复制机制展示了第 2 节中描述的理想的共识属性：只要集群中的大多数服务器正常运行，Raft 可以接受，复制，应用新的日志条目；在正常情况下，可以使用单轮 RPC 将一条新条目复制到集群的大部分节点，并且一个运行缓慢的服务器节点不会影响性能。

### 5.4 安全性

前面的章节描述了 Raft 如何选举领导者以及复制日志条目。然而，到目前为止描述的机制不足以确保每个状态机完全按照顺序执行。比如，一个跟随者在领导者提交一个日志条目时可能不可用，然后，它可以被选为领导者，并用新条目覆盖这些条目；结果是不同的状态机可能执行不同的命令序列。

这一节通过增加对服务器可能被选举为领导者的一个限制来完成 Raft 算法。该限制确保给定某个任期的情况下，该任期的领导者包含所有之前任期的所有已提交的条目（如图 3 描述的领导者完整的属性）。考虑到选举限制，然后使承诺的规则更加精确。最后，我们给出了领导者完整的证明草图，并展示了它如何指导复制状态机的正确行为。

#### 5.4.1 选举限制

在任何基于领导者的共识算法中，领导者最终必须存储所有已提交的日志条目。在一些像 Viewstamped Replication 的共识算法中，一个领导者可以被选举即使它最开始没有包含所有已提交的条目。这些算法包含额外的机制来识别缺失条目，并在选举过程中或之后不久将其发送给新领导者。不幸的是，这导致了相当多的额外机制和复杂性。Raft 使用一个简单的方法，它能够保证所有先前任期中已提交的条目在每个新领导者被当选时出现在新领导者的日志里，而不需要将这些条目传输给领导者。这意味着日志条目仅仅流向一个方向，即从领导者到跟随者，领导者从不在它们的日志中重写已经存在的条目。

Raft 使用投票过程阻止候选者赢得选举，除非其日志包含所有提交的条目。一个候选者为了被选举必须联系集群中的大部分节点，意味着每个已提交的条目必须至少出现在这些节点中的一个。如果候选者的日志至少与大多数节点的任何其他日志一样最新（“最新”定义如下），那么它将持有所有已提交的条目。RequestVote RPC 实现了这个限制：该 RPC 包含关于候选者的日志的信息，投票的人如果发现它自己的日志比候选者的日志更新，投票的人会拒绝投票。

Raft 通过比较日志里最后的条目的索引和任期来决定两个日志哪个更新。如果这两个日志拥有的最后一个条目的任期不一样，那任期大的日志是更新的。如果这两个日志以相同的任期结尾，那么谁的日志长度更大谁就最新（译者注：即相同任期，index 大的更新）。

#### 5.4.2 提交先前任期的条目

如 5.3 节所述，领导者知道，一旦条目存储在大多数服务器上，就会提交当前任期的条目。如果一个领导者在提交一个条目时崩溃，未来的领导者会尝试完成该条目的复制。然而，领导者不能立即断定，上一个术语中的条目一旦存储在大多数服务器上就已提交。图 8 阐述了一种情况，一个旧的日志条目虽然被存储在大多数节点上，任然可以被未来的领导者重写。

![Figure8](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure8.png)

*图 8 注：一个时间序列，显示领导者为什么不能从旧任期中的日志条目决定。在（a）中，S1 是领导者，在索引 2 的位置复制了日志条目。在（b）中，S1 宕机，S5 接受 S3，S4 和它自己的选票成为任期 3 的领导者并在日志中索引 2 的位置接受了不同的条目。在（c）中，S5 宕机，S1 重启并成为领导者，并继续复制日志。这时，来自任期 2 的日志条目已经在多数节点上被复制。但未提交。如果 S1 在（d）中宕机，S5 可以被选为领导者（来自 S2，S3，S4 的选票）并且用自己任期 3 的条目重写条目。然而，如果 S1 在宕机之前将自己任期的条目复制到多数节点上，如（e）所示，然后该条目被提交（ S5 无法当选）。这时日志中所有先前的条目也是提交的状态。*

为了消除图 8 中的问题，Raft 从不通过计算副本来提交以前任期中的日志条目。只有来自领导者当前任期的日志条目通过计算副本来提交；一旦一个来自当前任期的条目像这样被提交，然后由于日志匹配属性，所有先前的条目都是间接提交的。在某些情况下，领导者可以安全地断定提交了较旧的日志条目（例如条目被存储在每台节点上），但 Raft 为了简单起见采取了更为保守的方法。

Raft 在提交规则中带来了额外的复杂性，因为当领导者复制以前任期的条目时，日志条目保留其原始任期编号。在其他共识算法中，如果一个新的领导者从先前的“任期”中重新复制条目，则必须使用新的“任期编号”。Raft 的方法更容易推理日志条目，因为它们在一段时间内和跨日志保持相同的任期编号。此外，相比其他算法，Raft 的新领导者从先前任期发送更少的日志条目（其他算法在条目可以被提交之前，必须发送冗余的日志条目来对条目进行重写编号）。

#### 5.4.3 安全性论证

给出了完整的 Raft 算法，我们现在可以更精确地论证领导者完备性属性成立（这个论证基于安全性论证；9.2 小节）。我们假定领导者完备性属性不成立，然后反证。假定任期为 T 的领导者（leader-T）在它的任期提交一个日志条目，但这个日志条目没有被未来的领导者保存。考虑最小的任期 U > T，领导者（leader-U）没有保存该条目。

![Figure9](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure9.png)

*图 9 注：如果 S1（任期 T 的领导者）在它任期提交一个新的日志条目，S5 被选举为任期 U 的领导者，那么必然会有至少一个节点（S3）接受日志条目并且为 S5 投票。*

1. 已提交的条目不会出现在 leader-U 作为领导者时的日志中（领导者从不删除或者重写条目）。
2. leader-T 在集群中的多数节点复制条目，并且 leader-U 接收来自集群中多数节点的投票。因此，至少一个节点（“投票者”）接受了来自 leader-T 的条目并为 leader-U 投票，如图 9 所示。该投票中是引起矛盾的关键。
3. 投票者必须在为 leader-U 投票之前接受来自 leader-T 的已提交的条目；否则它将拒绝来自 leader-T 的 AppendEntries 请求（它的当前任期将高于 T）。
4. 投票者在为 leader-U 投票时仍然保存该条目，因为每个介入的领导者都包含该条目（假设），领导者从不移除条目，跟随者只移除与领导者冲突的条目。
5. 投票者将选票投给 leader-U，leader-U 的日志必须已经与投票的日志一样新。这引出了两个反证中的一个。
6. 首先，如果投票者和 leader-U 共享相同的最新的日志条目，leader-U 的日志必须至少与投票者的日志一样长，所以它的日志包含了选民日志中的每个条目。这是一个矛盾的地方，因为投票者包含已提交的条目并且 leader-U 是被假定不包含的。
7. 否则，leader-U 最新日志的任期必须大于投票者日志的任期。此外，它也大于任期 T，因为投票者的日志任期最少也是 T（它包含来自任期 T 已提交的日志）。创建 leaderU 的最后一个日志条目的早期领导者必须在其日志中包含提交的条目（假定）。然后通过日志匹配特性，leader-U 的日志必须包含已提交的日志，这是个矛盾。
8. 到这，完成了矛盾论证。所有大于 T 任期的领导者必须包含任期 T 内提交的所有的条目。
9. 日志匹配属性保证了未来领导者会包含非直接已提交的条目，例如图 8 （d）中的索引 2。

给出的领导者完备性属性，我们可以由图 3 证明状态机的安全性，其陈述的是如果一个节点已经将日志条目在它的状态机上应用到给定的索引上，那么没有其他节点应用不同的日志条目到该相同的索引上。服务器将日志条目应用到其状态机时，它的日志必须与领导者的日志相同，并且条目必须被提交。现在来考虑在最低任期任何节点应用一个给定的日志索引；日志完备性属性保证所有较高任期的领导者都将存储相同的日志条目，因此在以后任期中应用索引的服务器将应用相同的值。因此，状态机安全属性成立。

最后，Raft 要求节点按照日志索引的顺序应用条目。结合状态机安全属性，这意味着所有服务器将会按照相同的顺序应用相同日志条目到它们的状态机。

### 5.5 跟随者和候选人崩溃

直到现在，我们已经聚焦了领导者失败时的情况。跟随者和候选者的崩溃处理起来更简单。如果一个跟随者或者候选者崩溃，将来发送给它们的 RequestVote RPC 和 AppendEntries RPC 会失败。Raft 通过无限重试来处理这些失败问题；如果崩溃的服务器重启，那么这些 RPC 将会成功地完成。如果一个节点在响应 RPC 时崩溃，那么它会在重启时再次收到相同的 RPC。Raft 的 RPC 是幂等的，所以这不会产生影响。比如，如果一个跟随者接收了一个已经存在在它日志里的包含日志条目的 AppendEntries 请求，它会在新的请求中忽略这些条目。

### 5.6 计时与可用性

我们对 Raft 的要求之一是，安全不得依赖于时间：系统必须不能因为某些事件发生得或慢导致不符合预期而产生不正确的结果。然而，可用性（系统能够及时响应客户端的能力）是不可避免的得依靠时间。例如，如果消息交换时间比服务器崩溃之间的常见时间长，候选者不会坚持足够长的时间赢得选举；没有了一个稳定的领导者，Raft 无法进行。

领导者选举是 Raft 的一方面，其中时间很重要。Raft 能够选举和维护一个稳定的尽可能长的领导者，只要系统能够满足下面时间的要求：
$$
broadcastTime ≪ electionTimeout ≪ MTBF
$$
在这个不等式里，broadcastTime 是一个节点并行向集群里每个节点发生 RPC 并收到响应的平均时间；electionTimeout 是 5.2 小节里描述的选举超时时间；MTBF 是一个节点多次失败的平均时间。broadcastTime 应比 electionTimeout 小一个数量级，以便领导者能够可靠地发送阻止追随者开始选举所需的心跳消息；考虑到用于选举暂停的随机方法，这个不等式也使得分裂投票不太可能发生。electionTimeout 应该比 MTBF 小几个数量级，以便系统能够稳定的运行。当领导者崩溃，系统将在选举时间内变得不可用；我们希望这只代表总时间的一小部分。

broadcastTime 和 MTBF 是底层系统的属性，而 electionTimeout 是我们必须选择的。Raft 的 RPC 通常要求接收方将信息持久化到稳定的存储中，所以 broadcastTime 可能的范围是 0.5 毫秒到 20 毫秒之间，取决于存储技术。最后的结果是，electionTimeout 很可能是 10 毫秒到 500 毫秒之间。通常来说 MTBF 是几个月或者更长，这样很容易满足时间要求。

## 6 成员关系变化

到目前为止，我们假设集群配置（参与一致性算法的服务器集）是固定的。实际上，它会视情况修改配置，比如在节点失败时使用新的节点替换，或者改变复制程度。虽然可以通过将整个集群停机，更新配置文件，然后重启集群完成这些替换，但会使集群在改变过程中不可用。此外，如果这些过程有手动的步骤，它们可能会引发操作失误。为了避免这些问题，我们决定自动化配置更改，并将其纳入Raft共识算法。

为了使配置更改机制的安全，在过渡期间，绝不能出现两位领导人同时当选的情况。不幸的是，没有办法能够安全地保障节点从旧的配置直接切换到新的配置。而且不可能自动地一次性切换所有节点，所以该集群在转变的过程中是可预测的被分成独立的并具有多数节点的两部分（图 10）。

![Figure10](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure10.png)

*图 10 注：直接从一个配置切换到另外一个是不安全的，因为不同的节点会在不同的时间切换。这个例子中，集群从 3 个节点加到 5 个。不幸的是，有一个时间点在同一任期选举出了两个领导者，一个是以旧集群为多数的领导者（C-old）以及另外一个以新集群为多数的领导者（C-new）。*

为了保证安全性，配置的改变必须使用一个两阶段的方法。有各种方法去实现两阶段。比如，一些系统使用第一阶段去禁用旧配置目的是让它不能处理客户端请求；然后在第二阶段启用新的配置。在 Raft 中，集群首先切换到一个传统的配置，我们称为联合共识；一旦联合共识被提交，系统然后开始转变到新的配置上。联合共识结合了新旧两个配置：

- 在新旧两个配置中，日志条目被复制到所有节点上。
- 任一配置中的任何服务器都可以作为领导者。
- 一致（为了选举和条目承诺）要求从新旧配置分离开来。

联合共识允许单个服务器在不同时间在配置之间进行转换，而不影响安全性。更进一步，联合共识允许集群在变更配置的过程总持续响应客户端请求。

Raft 算法在复制的日志中使用特殊的条目来保存、交互集群配置；图 11 阐述了配置的更改过程。当领导者接收到从 C-new 改到 C-old 配置的请求，它为联合共识将配置保存成一条日志条目并使用先前描述的机制复制该条目。一旦一个节点将新的配置添加到它的日志里，就会使用该配置作为未来决策的基础（一个节点总会使用其日志里最新的配置，不管该条目是否提交）。这意味着领导者将使用 C-new,old 的规则来决定 C-old,new 的日志合适被提交。如果领导崩溃，一个新的领导者可以在 C-old 或 C-old,new 中选择出来，这取决于赢得选举的候选者是否已经接收到 C-old,new。无论如何，C-new 在此期间不能做出单方面决定。

![Figure11](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure11.png)

*图 11 注：配置更改的时间线。虚线展示配置条目被创建但没有没提交，实线展示最新的配置条目提交。领导者首先在它的日志里创建 C-old,new 配置条目并且将其提交到 C-old,new（C-old 的多数节点与 C-new 的大多数）。然后它创建 C -new 条目并将其提交到 C-new 的大多数。这样一来，C-old 和 C-new 都无法独立做出决策。*

一旦 C-old,new 被提交，C-old 或者 C-new 都不可以在未经对方批准的情况下做出决定，领导者完备性属性确保只有带有 C-old,new 日志条目可以被选举为领导者。现在，领导者可以安全地创建描述 C-new 的日志条目，并将其复制到集群。这个配置将会尽快的在每台节点上生效。当新的配置在基于 C-new 的规则上被提交，旧的配置变得无关并且不在新配置里的服务器可以被撤出集群。如图 11 所示，C-old 与 C-new 在任何时间点都不能单独做出决定；这保证了安全性。

重新配置还需要解决三个问题。第一个问题是新的服务器可能在初始阶段没有保存任何日志条目。如果它们以这个状态被加入到集群，它们会花费一段时间来跟上进度，在这期间它可能没法提交任何新的日志条目。为了避免可用性问题，Raft 在配置更改之前引入了一个额外的阶段，新的节点以不投票的成员加入到集群（领导者将日志条目复制给它们，但它们此时还不属于集群的大多数）。一旦新的节点的数据量跟上了集群中的其他节点，重新配置就按照上面描述的那样进行。

第二个问题是集群的领导者可能不是新配置的一部分。这种情况下，一旦提交了 C-new 日志条目，领导者进行下台（回到跟随者状态）。这意味着会存在一段时间（正在提交 C-new）领导者正在管理一个集群但并不包括它自己；它复制日志条目但没有把自己考虑到大多数中。该领导者的转换在 C-new 被提交时发生，因为这是第一个时间节点新的配置可以独立地操作（总是可以从 C-new 中选择领导者）。在此之前，可能只有来自 C-old 的服务器才能当选领导者。

第三个问题是移除节点（不在 C-new 的节点）可以中断集群。这些节点将不会接收心跳，所以它们会超时并且开始新的选举。它们之后会携带新的任期编号发送 RequestVote RPC，这会使当前领导者转换为跟随者状态。一个新的领导者会最终被选举出来，但被移除的节点会再次超时，该过程会重复，导致较差的可用性。

为了防止这个问题，当服务器认为存在当前领导者时，会忽略 RequestVote RPC。具体来说，如果服务器在听取当前领导者最短选举超时时间内收到 RequestVote RPC，它不会更新它的任期或者为它投票。这不会影响正常选举，每个节点在开始选举之前等待一个最小的选举超时时间。这样，它在移除节点期间避免了中断：如果一个领导者能够获得其集群的心跳，那么它就不会被更大的任期编号所取代。

## 7 日志压缩

Raft 的日志在正常操作期间增长，以合并更多客户端请求，但在实际的系统中，它不能无限的增长。随着日志变长，它需要占用更多的空间，并花费更多的时间重放。没有一些机制来抛弃淘汰的日志中累计的信息，这最终会引起可用性问题。

快照是最简单的压缩方法。在快照中，整个当前系统状态被写入到稳定存储上的快照中，然后直到该状态前的所有日志被废弃。Chubby 与 ZooKeeper 使用了快照，下面剩余的小节用来描述 Raft 中的快照。

也可以使用增量压缩方法，例如日志清理和日志结构合并树。这些一次只会对少部分数据进行操作，因此，随着时间的推移，他们会更均匀地分散压缩荷载。它们首先选择一个数据区域，该区域积累了许多删除和覆盖的对象，然后它们向该区域重写存活的数据并释放该区域。与快照相比，这需要更多的机制和复杂性，快照通过始终操作整个数据集来简化问题。虽然日志清理需要修改 Raft，但状态机可以使用与快照相同的接口实现 LSM 树。

![Figure12](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure12.png)

*图 12 注：一个服务器使用快照代替其日志中（索引 1-5）已提交的条目，该快照恰好存储当前状态（在这个例子中的变量 x 和 y）。快照最后包含的索引和任期用于将快照定位在条目 6 之前的日志中。*

图 12 展示了 Raft 中关于快照的基本想法。每台节点独立地操作快照，在其日志覆盖已提交的条目。大多数工作包括状态机将其当前状态写入快照。Raft 同样包含一小部分的元数据在快照中：last included index 是快照替换的日志中最后一个条目的索引（状态机最后应用的条目），last included term 是这个条目的任期。保留这些条目以支持快照后第一个日志条目的 AppendEntries 一致性检查，因为该条目需要一个先前的日志条目和任期。为了启用集群成员变化（第 6 节），快照还包括日志中截至上次包含索引的最新配置。一旦一台服务器完成写入一个快照，它可以删除上一个包含的索引中的所有日志条目，以及之前的任何快照。

尽管服务器能独立的保存快照，但领导者必须偶尔向落后的追随者发送快照。当领导者已经抛弃了需要发送给跟随者的下一个日志条目时，就会发生这种情况。幸运的是，这种情况不太可能出现在正常的操作中：一个紧跟着领导者的跟随者可能已经有了这个条目。然而，一个例外地运行缓慢的跟随者或者一个加入集群的新机器（第 6 节）不会。让这样的追随者跟上最新的进度的方法是领导者通过网络向其发送快照。

![Figure13](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure13.png)

*图 13 注：一个 InstallSnapshot RPC 概览。快照被分为小块来传输；这为跟随者提供了每个块的生命迹象，因此它可以重置其选举计时器*

领导者使用新的 RPC，称为 InstallSnapshot，用来向落后太多的跟随者发送快照；如图 13。当一个跟随者接收到该 RPC 携带的快照，它必须决定如何对待它存在的日志条目。通常快照会包含接收者日志中不存在的新信息。这种情况下，跟随者抛弃它的日志条目；它全部被快照取代，并且可能有与快照冲突的未提交条目。如果跟随者收到描述其日志前缀的快照（由于重新传输或错误），则快照所涵盖的日志条目将被删除，但快照之后的条目仍然有效，必须保留。

这种快照方法背离了 Raft 的强领导原则，因为跟随者可以在领导者不知情的情况下拍摄快照。然而，我们认为这种偏离是合理的。虽然拥有领导者有助于避免在达成共识时做出冲突的决定，但在快照时已经达成共识，所以没有决定冲突。数据仍然是从领导者流向跟随者，只是跟随者现在可以重新组织它们的数据。

我们考虑了另一种基于领导者的方法，其中只有领导者会创建一个快照，然后它将这个快照发送给它的每个追随者。然而，这存在两个缺点。第一，给每个跟随者发送日志会浪费网络带宽并且减慢快照的进程。每个跟随者已经拥有了需要处理它自己快照的信息，并且服务器从其本地状态生成快照通常比通过网络发送和接收快照便宜得多。第二，领导者的实现将会更复杂。例如，领导者需要将快照发送给跟随者，同时将新的日志条目复制给它们，以免阻塞新的客户端请求。

还有两个问题影响快照的性能。第一，服务器必须决定何时进行快照。如果一个服务器很频繁地进行快照，会浪费磁盘带宽和能量；如果不频繁进行快照，会有耗尽存储空间的风险，并增加了在重新启动期间重播日志所需的时间。一个简单的策略是当日志达到固定字节大小时进行快照。如果将此大小设置为远大于快照的预期大小，则快照的磁盘带宽开销将很小。

第二个性能问题是写快照会耗费大量的时间，我们并不想因此拖延了正常的操作。解决方法是用写时复制技术目的是新的更新可以不会对快照写入造成影响而被接受。例如，使用功能数据结构构建的状态机自然支持这一点。或者，操作系统的写时复制支持（例如，Linux 上的 fork）可用于创建整个状态机的内存快照（我们的实现使用这种方法）。

## 8 客户端交互

这一节描述客户端如何与 Raft 交互，包括客户端查找集群中的领导者和 Raft 如何支持线性化语义。这些问题适用于所有以共识为基础的系统，并且 Raft 的处理这些问题的方法与其他系统相似。

Raft 的客户端将所有请求发送给领导者。当一个客户端首次启动，它随机选择一个节点连接。如果客户端首次选择不是领导者，那么该服务器将会拒绝客户端的请求并将近期接收到的领导者的信息提供给客户端（附加条目请求包含领导者的网络地址）。如果领导者崩溃，客户端的请求会超时，客户端然后再次尝试随机选择服务器。

Raft 的目标是实现可线性化的语义（每个操作似乎在其调用和响应之间的某个时间点立即执行一次）。然而，目前为止 Raft 可以多次执行一个命令：比如，领导者在提交日志条目之后崩溃，但并未响应客户端，客户端会向新的领导者发起重试，导致它被第二次执行。解决办法是客户端给每个命令分配一个唯一的序号。然后，状态机跟踪为每个客户端处理的最新序列号，以及相关的响应。如果它接收到一条序列号已经被执行过的命令，它不回重新执行并立即响应该请求。

只读的操作可以处理成无需写入任何日志。然而，如果没有额外的措施，这将面临返回陈旧数据的风险，因为响应请求的领导者可能已被它不知道的新领导者取代。线性读必定不回返回旧数据，Raft 需要两个额外的预防措施来保证该特性，并且不使用日志。首先，一个领导者必须拥有关于条目被提交的最新信息。领导者完备性属性保证一个领导者拥有所有已提交的条目，但在它任期的最开始，它可能不知道哪些是。为了弄清楚，它需要在它的任期提交一个条目。Raft 通过在它任期开始时每个领导者提交一个空白的无操作的条目到它的日志来处理这个问题。第二，领导者必须在处理只读请求之前检查它是否已被撤销（如果选举了一个更新的领导人，它的信息可能是陈旧的）。Raft 通过领导者在响应只读请求之前与集群中的大多数交换心跳来处理这个问题。或者，领导者可以依靠心跳机制来提供一种租约形式，但这将依赖于安全时间（它假设有界时钟偏差）。

## 9 实现与评估

我们已经将 Raft 实现为复制状态机的一部分，该状态机存储 RAMCloud 的配置信息并协助 RAMCloud 协调器的故障转移。Raft 的实现包含 2000 行 C++ 代码，不包含测试、注释或空白行。源码是免费可用的。根据本文的草稿，还有大约 25 个独立的第三方开源实现处于不同的开发阶段。同样，各类公司正在开发基于 Raft 的系统。

该节的剩余部分使用三个标准来评估 Raft：可理解性，正确性以及性能。

#### 9.1 可理解性

为了衡量 Raft 相对于 Paxos 的可理解性，我们使用斯坦福大学高级操作系统课程和加州大学伯克利分布式计算课程的高年级本科生和研究生进行了一项实验研究。我们录制了关于 Raft 和另外一个 Paxos 的录像课程，并创建了相应的测验。Raft 课程覆盖了这片文章除日志压缩的内容；Paxos 课程涵盖了足够的材料来创建等效的复制状态机，包括单法令 Paxos、多法令 Paxos、重新配置和实践中需要的一些优化（例如领导者选举）。测试题测试对算法基本的理解，还要求学生对角落案例进行推理。每个学生观看一个视频，参加对应的测试，观看第二个视频，再参加第二个测试。大约一半的参与者先做 Paxos 部分，另一半先做 Raft 部分，以说明从研究第一部分获得的表现和经验的个体差异。我们在每个测试上比较了参与者的分数来决定参与者是否对 Raft 的理解更好。

我们试图让 Paxos 和 Raft 之间的比较尽可能公平。实验在两个方面对 Paxos 有利：43 名参与者中的 15 名报告说对 Paxos 有过相关经验，并且 Paxos 的视频长度比 Raft 的视频长度长 14%。如表 1 总结所示，我们已采取措施减少潜在的偏见来源。 我们所有的材料都可供审查。

![Table1](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Table1.png)

平均而言，参与者的 Raft 测试得分比 Paxos 测试得分高 4.9 分（在可能的 60 分中，平均 Raft 得分为 25.7，平均 Paxos 得分为 20.8）；图 14 展示了他们各自得分。配对 t 检验表明，95% 的置信度下，Raft 分数的真实分布比 Paxos 分数的真实分配平均值至少大 2.5 分。

![Figure14](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure14.png)

*图 14 注：比较 43 名参与者在 Raft 和 Paxos 测验中的表现的散点图。对角线上方的点代表 Raft 得分较高的参与者。*

我们也创建了一个线性回归模型，基于 3 个因素来预测新学生的测试分数：他们参加的测试，之前对 Paxos 的经验程度，他们学习这些算法的顺序。该模型预测，测验的选择会产生 12.5 分的差异，有利于 Raft。这比之前观察的 4.9 分的差异更高，因为许多学生先前拥有相关 Paxos 算法的经验，这对 Paxos 的帮助很大，而对 Raft 的帮助则略少。奇怪的是，该模型还预测已经参加过 Paxos 测验的人在 Raft 上的得分会低 6.3 分； 尽管我们不知道为什么，但这似乎具有统计学意义。

我们还在测验后对参与者进行了调查，以了解他们认为哪种算法更容易实现或解释；这些结果如图 15 展示。绝大多数参与者报告说 Raft 会更容易实施和解释（每个问题 41 个问题中的 33 个）。然而，这些自我报告的感受可能不如参与者的测验分数可靠，并且参与者可能因我们假设 Raft 更容易理解的知识而产生偏见。关于 Raft 用户研究的详细讨论见 [31]。

![Figure15](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure15.png)

*图 15 注：使用 5 点量表，参与者被问及（左）他们认为哪种算法更容易在功能正常、正确和高效的系统中实现，以及（右）哪种算法更容易向 CS 研究生解释。*

#### 9.2 正确性

我们已经在第 5 节中为共识算法机制开发了一个正式的规范以及一个安全性证明。正式规范使用 TLA+ 规范语言使图 2 中总结的信息完全精确。它长约 400 行，作为证明的主题。它对于任何人想要实现 Raft 也是有用的。我们已经使用 TLA 证明系统机械地证明了日志完整性属性。然而，这个证明依赖于没有经过机械检查的不变量（例如，我们没有证明规范的类型安全）。此外，我们编写了状态机安全属性的非正式证明 [31]，它是完整的（仅依赖于规范）且相对精确（大约 3500 字长）。

#### 9.3 性能

Raft 的性能与其他共识算法如 Paxos 相似。性能最重要的情况是当一个已建立的领导者复制新的日志条目时。Raft 使用最小信息序号来完成（从领导者到一半集群的单次往返）。也有可能能够进一步提高 Raft 的性能。比如，它可以轻松支持批处理和流水线请求，以获得更高的吞吐量和更低的延迟。文献中针对其他算法提出了各种优化；其中很多方法都可以应用到 Raft 上，但我们把这个作为未来的工作。

我们使用 Raft 实现来衡量 Raft 的领导者选举算法的性能并回答两个问题。第一，选举过程是否很快收敛？ 第二，领导者崩溃后能达到的最小宕机时间是多少？

为了衡量领导者选举，我们重复地将由 5 个机器组成的集群中的领导者崩溃，并对其检测崩溃和选举新的领导者计时（见图 16）。为了产生最坏的情况，每个试验中的服务器具有不同的日志长度，因此一些候选人没有资格成为领导者。此外，为了鼓励分裂投票，我们的测试脚本在终止其进程之前触发了来自领导者的心跳 RPC 的同步广播（这近似于领导者在崩溃之前复制新日志条目的行为）。领导者在其心跳间隔内均匀随机崩溃，这是所有测试的最小选举超时的一半。 因此，最小的可能停机时间大约是最小选举超时的一半。

![Figure16](https://github.com/notayessir/blog/blob/main/images/raft-consensus/Figure16.png)

*图 16 注：检测和更换崩溃领导者的时间。上图改变了选举超时的随机性，下图缩放了最小选举超时。每行代表 1000 次试验（“150 150ms”的 100 次试验除外），并对应于选举超时的特定选择； 例如，“150–155ms”表示选举超时时间是在 150ms 到 155ms 之间随机均匀选择的。测量是在一个由五台服务器组成的集群上进行的，广播时间约为 15 毫秒。 九台服务器集群的结果是相似的。*

图 16 中的顶部图表显示，选举超时中的少量随机化足以避免选举中的分裂投票。在没有随机性的情况下，由于许多分裂投票，在我们的测试中，领导人选举持续超过 10 秒。仅添加 5 毫秒的随机性就可以显着提高停机时间，平均停机时间为 287 毫秒。使用更多的随机性可以改善最坏情况的行为：随机性为 50 毫秒，最坏情况的完成时间（超过 1000 次试验）为 513 毫秒。

图 16 中的底部图表显示，可以通过减少选举超时来减少停机时间。选举超时时间为 12-24 毫秒，平均只需要 35 毫秒来选举领导者（最长的试验需要 152 毫秒）。然而，将超时时间降低到超过这一点违反了 Raft 的时间要求：领导者很难在其他服务器开始新的选举之前广播心跳。 这可能会导致不必要的领导者变更和整体系统可用性降低。 我们建议使用保守的选举超时时间，例如 150–300 毫秒； 此类超时不太可能导致不必要的领导者更改，并且仍将提供良好的可用性。

## 10 相关工作

已经有许多与共识算法相关的出版物，其中许多属于以下类别之一：

- Lamport 对 Paxos [15] 的原始描述，并试图更清楚地解释它 [16, 20, 21]。
- Paxos 的详细说明，它填补了缺失的细节并修改了算法，为实现提供了更好的基础 [26,39,13]。
- 实现共识算法的系统，例如 Chubby [2, 4]、ZooKeeper [11, 12] 和 Spanner [6]。 Chubby 和 Spanner 的算法尚未详细公布，尽管两者都声称基于 Paxos。 ZooKeeper 的算法已经发表了更详细的内容，但它与 Paxos 有很大的不同。
- 可应用于 Paxos [18、19、3、25、1、27] 的性能优化。
- Oki 和 Liskov 的 Viewstamped Replication (VR)，一种与 Paxos 大约同时开发的共识替代方法。 最初的描述 [29] 与分布式事务协议交织在一起，但核心共识协议在最近的更新 [22] 中已经分离。 VR 使用与 Raft 有许多相似之处的基于领导者的方法。

Raft 与 Paxos 最大的区别是 Raft 的强领导：Raft 使用领导选举作为共识协议的重要组成部分。并且它将尽可能多的功能集中在领导者身上。这种方法产生了一种更简单、更容易理解的算法。例如，在 Paxos 中，leader 选举与基本共识协议是正交的：它仅用作性能优化，而不是达成共识所必需的。 然而，这导致了额外的机制：Paxos 包括用于基本共识的两阶段协议和用于领导者选举的单独机制。 相比之下，Raft 将领导者选举直接纳入共识算法，并将其用作共识的两个阶段中的第一个阶段。 这导致机制比 Paxos 少。

与 Raft 一样，VR 和 ZooKeeper 都是基于领导者的，因此与 Paxos 相比，具有 Raft 的许多优势。 然而，Raft 的机制不如 VR 或 ZooKeeper，因为它最小化了非领导者的功能。 例如，Raft 中的日志条目仅流向一个方向：从 AppendEntries RPC 中的领导者向外流动。 在 VR 中，日志条目是双向流动的（领导者可以在选举过程中收到日志条目）； 这导致额外的机制和复杂性。 ZooKeeper 的发布描述也将日志条目传输到领导者和从领导者那里传输，但实现显然更像 Raft。

Raft 的消息类型比我们知道的任何其他基于共识的日志复制算法都要少。例如，我们计算了 VR 和 ZooKeeper 用于基本共识和成员更改的消息类型（不包括日志压缩和客户端交互，因为这些几乎独立于算法）。VR 和 ZooKeeper 各自定义了 10 种不同的消息类型，而 Raft 只有 4 种消息类型（两个 RPC 请求及其响应）。 Raft 的消息比其他算法更密集一些，但它们总体上更简单。 另外，VR 和 ZooKeeper 是从领导者变更时传输整个日志的角度来描述的； 将需要额外的消息类型来优化这些机制，以便它们实用。

Raft 强大的领导方法简化了算法，但它排除了一些性能优化。 例如，平等主义 Paxos (EPaxos) 可以在某些条件下通过无领导方法获得更高的性能 [27]。 EPaxos 利用状态机命令中的交换性。 只要提议的其他命令与它同时通勤，任何服务器都可以通过一轮通信来提交命令。 但是，如果同时提出的命令不相互通勤，EPaxos 需要额外的一轮通信。 因为任何服务器都可以提交命令，EPaxos 可以很好地平衡服务器之间的负载，并且能够在 WAN 设置中实现比 Raft 更低的延迟。 然而，它显着增加了 Paxos 的复杂性。

在其他工作中已经提出或实施了几种不同的集群成员更改方法，包括 Lamport 的原始提案 [15]、VR [22] 和 SMART [24]。 我们为 Raft 选择了联合共识方法，因为它利用了共识协议的其余部分，因此成员变更几乎不需要额外的机制。 Lamport 的基于 α 的方法不是 Raft 的选择，因为它假设可以在没有领导者的情况下达成共识。 与 VR 和 SMART 相比，Raft 的重配置算法的优势在于可以在不限制正常请求处理的情况下发生成员变化； 相比之下，VR 在配置更改期间停止所有正常处理，SMART 对未完成请求的数量施加类似 α 的限制。 Raft 的方法也比 VR 或 SMART 增加了更少的机制。

## 11 总结

算法的设计通常以正确性、效率和/或简洁性为主要目标。尽管这些都是有价值的目标，但我们相信可理解性同样重要。在开发人员将算法转化为实际实现之前，其他任何目标都无法实现，这将不可避免地偏离并扩展已发布的形式。除非开发人员对算法有深刻的理解并且可以对它产生直觉，否则他们很难在实现中保留其理想的属性。

在本文中，我们讨论了分布式共识的问题，在这个问题上，一种被广泛接受但难以理解的算法 Paxos 多年来一直在挑战学生和开发人员。 我们开发了一种新算法 Raft，我们已经证明它比 Paxos 更容易理解。我们也相信 Raft 为系统搭建提供了更好的基础。 将可理解性作为主要设计目标改变了我们设计 Raft 的方式； 随着设计的进展，我们发现自己重复使用了一些技术，例如分解问题和简化状态空间。 这些技术不仅提高了 Raft 的可理解性，而且更容易让自己相信它的正确性。

## 12 致谢（省略）

## 引用（省略） 